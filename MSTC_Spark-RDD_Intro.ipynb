{"cells":[{"cell_type":"markdown","source":["# Introduction to <font color=blue>pySpark</font> https://spark.apache.org/\n\n![Image of Sklearn](https://spark.apache.org/images/spark-logo-trademark.png)"],"metadata":{}},{"cell_type":"markdown","source":["## RDD creation"],"metadata":{}},{"cell_type":"markdown","source":["**Resilient Distributed Dataset** or **RDD**. An RDD is a distributed collection of elements.\n\n* Creating new RDDs, **transforming** existing RDDs, or calling **actions** on RDDs to compute a result.\n\n* Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them."],"metadata":{}},{"cell_type":"markdown","source":["### FIRST check SparkContext is available"],"metadata":{}},{"cell_type":"code","source":["sc"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>&lt;SparkContext master=local[8] appName=Databricks Shell&gt;\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["#### From Spark 2.2... only *SparkSession*"],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">2</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.session.SparkSession at 0x7fb463ee7c90&gt;\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### GET SparkContext configuration will be \"local\" see spark.master\n\n* ### <font color=red>local[*]</font> means Spark locally with as many worker threads as logical cores on your machine."],"metadata":{}},{"cell_type":"code","source":["sc._conf.getAll()"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">4</span><span class=\"ansired\">]: </span>\n[(u&apos;spark.databricks.preemption.enabled&apos;, u&apos;true&apos;),\n (u&apos;spark.driver.tempDirectory&apos;, u&apos;/local_disk0/tmp&apos;),\n (u&apos;spark.hadoop.fs.adl.impl.disable.cache&apos;, u&apos;true&apos;),\n (u&apos;spark.hadoop.fs.s3a.connection.maximum&apos;, u&apos;200&apos;),\n (u&apos;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&apos;, u&apos;0&apos;),\n (u&apos;spark.shuffle.reduceLocality.enabled&apos;, u&apos;false&apos;),\n (u&apos;spark.sql.streaming.checkpointFileManagerClass&apos;,\n  u&apos;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&apos;),\n (u&apos;spark.databricks.clusterUsageTags.driverNodeType&apos;, u&apos;dev-tier-node&apos;),\n (u&apos;spark.hadoop.spark.sql.sources.outputCommitterClass&apos;,\n  u&apos;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&apos;),\n (u&apos;spark.streaming.driver.writeAheadLog.allowBatching&apos;, u&apos;true&apos;),\n (u&apos;spark.databricks.clusterSource&apos;, u&apos;UI&apos;),\n (u&apos;spark.hadoop.hive.server2.transport.mode&apos;, u&apos;http&apos;),\n (u&apos;spark.databricks.driverNodeTypeId&apos;, u&apos;dev-tier-node&apos;),\n (u&apos;spark.sql.parquet.compression.codec&apos;, u&apos;snappy&apos;),\n (u&apos;spark.databricks.clusterUsageTags.driverContainerPrivateIp&apos;,\n  u&apos;10.172.250.152&apos;),\n (u&apos;spark.databricks.clusterUsageTags.driverInstanceId&apos;,\n  u&apos;i-072a425d7d1ca3d22&apos;),\n (u&apos;spark.databricks.clusterUsageTags.driverInstancePrivateIp&apos;,\n  u&apos;10.172.247.108&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&apos;, u&apos;false&apos;),\n (u&apos;spark.eventLog.enabled&apos;, u&apos;false&apos;),\n (u&apos;spark.hadoop.fs.wasb.impl&apos;,\n  u&apos;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&apos;),\n (u&apos;spark.repl.class.uri&apos;,\n  u&apos;spark://ip-10-172-250-152.us-west-2.compute.internal:42544/classes&apos;),\n (u&apos;spark.executor.tempDirectory&apos;, u&apos;/local_disk0/tmp&apos;),\n (u&apos;spark.databricks.workerNodeTypeId&apos;, u&apos;dev-tier-node&apos;),\n (u&apos;spark.hadoop.fs.abfs.impl&apos;,\n  u&apos;shaded.databricks.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&apos;),\n (u&apos;spark.hadoop.mapred.output.committer.class&apos;,\n  u&apos;com.databricks.backend.daemon.data.client.DirectOutputCommitter&apos;),\n (u&apos;spark.app.id&apos;, u&apos;local-1543271880084&apos;),\n (u&apos;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&apos;, u&apos;2&apos;),\n (u&apos;spark.hadoop.hive.server2.thrift.http.port&apos;, u&apos;10000&apos;),\n (u&apos;spark.sql.allowMultipleContexts&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterEbsVolumeSize&apos;, u&apos;0&apos;),\n (u&apos;spark.home&apos;, u&apos;/databricks/spark&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterTargetWorkers&apos;, u&apos;0&apos;),\n (u&apos;spark.sql.warehouse.dir&apos;, u&apos;/user/hive/warehouse&apos;),\n (u&apos;spark.hadoop.hive.server2.idle.operation.timeout&apos;, u&apos;7200000&apos;),\n (u&apos;spark.task.reaper.enabled&apos;, u&apos;true&apos;),\n (u&apos;spark.storage.memoryFraction&apos;, u&apos;0.5&apos;),\n (u&apos;spark.databricks.session.share&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterResourceClass&apos;, u&apos;default&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterFirstOnDemand&apos;, u&apos;0&apos;),\n (u&apos;spark.driver.maxResultSize&apos;, u&apos;4g&apos;),\n (u&apos;spark.databricks.delta.multiClusterWrites.enabled&apos;, u&apos;true&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterSku&apos;, u&apos;STANDARD_SKU&apos;),\n (u&apos;spark.worker.cleanup.enabled&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.clusterUsageTags.enableCredentialPassthrough&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&apos;,\n  u&apos;ebs_volume_type: GENERAL_PURPOSE_SSD\\n&apos;),\n (u&apos;spark.databricks.clusterUsageTags.enableJdbcAutoStart&apos;, u&apos;true&apos;),\n (u&apos;spark.executor.extraJavaOptions&apos;,\n  u&apos;-XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Ddatabricks.serviceName=spark-executor-1 -javaagent:/databricks/DatabricksAgent.jar -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterId&apos;, u&apos;1126-223735-heals500&apos;),\n (u&apos;spark.hadoop.fs.wasb.impl.disable.cache&apos;, u&apos;true&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterEbsVolumeType&apos;,\n  u&apos;GENERAL_PURPOSE_SSD&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterLogDestination&apos;, u&apos;&apos;),\n (u&apos;spark.cleaner.referenceTracking.blocking&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.delta.preview.enabled&apos;, u&apos;true&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterState&apos;, u&apos;Pending&apos;),\n (u&apos;spark.databricks.tahoe.logStore.azure.class&apos;,\n  u&apos;com.databricks.tahoe.store.AzureLogStore&apos;),\n (u&apos;spark.hadoop.fs.azure.skip.metrics&apos;, u&apos;true&apos;),\n (u&apos;spark.driver.port&apos;, u&apos;42544&apos;),\n (u&apos;spark.master&apos;, u&apos;local[8]&apos;),\n (u&apos;spark.scheduler.mode&apos;, u&apos;FAIR&apos;),\n (u&apos;spark.driver.host&apos;, u&apos;ip-10-172-250-152.us-west-2.compute.internal&apos;),\n (u&apos;spark.databricks.delta.logStore.crossCloud.fatal&apos;, u&apos;true&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterWorkers&apos;, u&apos;0&apos;),\n (u&apos;spark.files.fetchFailure.unRegisterOutputOnHost&apos;, u&apos;true&apos;),\n (u&apos;spark.hadoop.fs.s3n.impl&apos;, u&apos;com.databricks.s3a.S3AFileSystem&apos;),\n (u&apos;spark.databricks.clusterUsageTags.enableSqlAclsOnly&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterEbsVolumeCount&apos;, u&apos;0&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterNumSshKeys&apos;, u&apos;0&apos;),\n (u&apos;spark.databricks.tahoe.logStore.aws.class&apos;,\n  u&apos;com.databricks.tahoe.store.S3LockBasedLogStore&apos;),\n (u&apos;spark.speculation.quantile&apos;, u&apos;0.9&apos;),\n (u&apos;spark.files.overwrite&apos;, u&apos;true&apos;),\n (u&apos;spark.shuffle.manager&apos;, u&apos;SORT&apos;),\n (u&apos;spark.sql.hive.metastore.sharedPrefixes&apos;,\n  u&apos;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&apos;),\n (u&apos;spark.databricks.io.directoryCommit.enableLogicalDelete&apos;, u&apos;false&apos;),\n (u&apos;spark.executor.extraClassPath&apos;,\n  u&apos;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/api-base--api-base_java-spark_2.3_2.11_deploy.jar:/databricks/jars/api-base--api-base-spark_2.3_2.11_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_2.3_2.11_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_2.3_2.11_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_2.3_2.11_deploy.jar:/databricks/jars/common--client--client-spark_2.3_2.11_deploy.jar:/databricks/jars/common--common-spark_2.3_2.11_deploy.jar:/databricks/jars/common--credentials--credentials-spark_2.3_2.11_deploy.jar:/databricks/jars/common--hadoop--hadoop-spark_2.3_2.11_deploy.jar:/databricks/jars/common--jetty--client--client-spark_2.3_2.11_deploy.jar:/databricks/jars/common--lazy--lazy-spark_2.3_2.11_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--path--path-spark_2.3_2.11_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_2.3_2.11_deploy.jar:/databricks/jars/common--storage--storage-spark_2.3_2.11_deploy.jar:/databricks/jars/daemon--data--client--client-spark_2.3_2.11_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_2.3_2.11_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_2.3_2.11_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_2.3_2.11_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_2.3_2.11_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_2.3_2.11_deploy.jar:/databricks/jars/extern--extern-spark_2.3_2.11_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/jsonutil--jsonutil-spark_2.3_2.11_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_2.3_2.11_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_2.3_2.11_deploy.jar:/databricks/jars/s3commit--client--client-spark_2.3_2.11_deploy.jar:/databricks/jars/s3commit--common--common-spark_2.3_2.11_deploy.jar:/databricks/jars/s3--s3-spark_2.3_2.11_deploy.jar:/databricks/jars/secret-manager--api--api-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--common--spark-common-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--driver--display--display-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--driver--driver-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--driver--events-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_2.3_2.11_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--maven-trees--spark_2.3--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.313.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.carrotsearch--hppc--com.carrotsearch__hppc__0.7.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.chuusai--shapeless_2.11--com.chuusai__shapeless_2.11__2.3.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.7.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.databricks--dbml-local_2.11--com.databricks__dbml-local_2.11__0.4.1-db1-spark2.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.databricks--dbml-local_2.11-tests--com.databricks__dbml-local_2.11-tests__0.4.1-db1-spark2.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__3.0.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.fasterxml.jackson.module--jackson-module-scala_2.11--com.fasterxml.jackson.module__jackson-module-scala_2.11__2.6.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.3.2-2.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.github.rwl--jtransforms--com.github.rwl__jtransforms__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.h2database--h2--com.h2database__h2__1.3.174.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.jamesmurty.utils--java-xmlbuilder--com.jamesmurty.utils__java-xmlbuilder__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.mchange--c3p0--com.mchange__c3p0__0.9.5.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.mchange--mchange-commons-java--com.mchange__mchange-commons-java__0.2.10.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.microsoft.sqlserver--mssql-jdbc--com.microsoft.sqlserver__mssql-jdbc__6.2.2.jre8.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.7.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-beanutils--commons-beanutils-core--commons-beanutils__commons-beanutils-core__1.8.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-net--commons-net--commons-net__commons-net__2.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.twitter--chill_2.11--com.twitter__chill_2.11__0.8.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.twitter--chill-java--com.twitter__chill-java__0.8.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.6.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.5.9.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.vlkan--flatbuffers--com.vlkan__flatbuffers__1.2.0-3f79e055.jar:/databricks/jars/spark--maven-trees--spark_2.3--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.airlift--aircompressor--io.airlift__aircompressor__0.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.netty--netty-all--io.netty__netty-all__4.1.17.Final.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.netty--netty--io.netty__netty__3.9.9.Final.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.0.16.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.0.16.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.prometheus--simpleclient--io.prometheus__simpleclient__0.0.16.jar:/databricks/jars/spark--maven-trees--spark_2.3--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.0.16.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.annotation--javax.annotation-api--javax.annotation__javax.annotation-api__1.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.ws.rs--javax.ws.rs-api--javax.ws.rs__javax.ws.rs-api__2.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/spark--maven-trees--spark_2.3--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--jline--jline--jline__jline__2.11.jar:/databricks/jars/spark--maven-trees--spark_2.3--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/spark--maven-trees--spark_2.3--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/spark--maven-trees--spark_2.3--net.hydromatic--eigenbase-properties--net.hydromatic__eigenbase-properties__1.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--net.iharder--base64--net.iharder__base64__2.3.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--net.java.dev.jets3t--jets3t--net.java.dev.jets3t__jets3t__0.9.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--net.razorvine--pyrolite--net.razorvine__pyrolite__4.13.jar:/databricks/jars/spark--maven-trees--spark_2.3--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.6.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--net.snowflake--spark-snowflake_2.11--net.snowflake__spark-snowflake_2.11__2.4.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__0.8.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.arrow--arrow-memory--org.apache.arrow__arrow-memory__0.8.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__0.8.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.7.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.avro--avro-ipc-tests--org.apache.avro__avro-ipc-tests__1.7.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.7.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.avro--avro--org.apache.avro__avro__1.7.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.calcite--calcite-avatica--org.apache.calcite__calcite-avatica__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.calcite--calcite-core--org.apache.calcite__calcite-core__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.calcite--calcite-linq4j--org.apache.calcite__calcite-linq4j__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.4.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.orc--orc-core-nohive--org.apache.orc__orc-core-nohive__1.4.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.orc--orc-mapreduce-nohive--org.apache.orc__orc-mapreduce-nohive__1.4.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.8.3-databricks2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.8.3-databricks2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.8.3-databricks2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.3.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.8.3-databricks2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.8.3-databricks2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.xbean--xbean-asm5-shaded--org.apache.xbean__xbean-asm5-shaded__4.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.bouncycastle--bcprov-jdk15on--org.bouncycastle__bcprov-jdk15on__1.58.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.codehaus.jackson--jackson-jaxrs--org.codehaus.jackson__jackson-jaxrs__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.codehaus.jackson--jackson-xc--org.codehaus.jackson__jackson-xc__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-plus--org.eclipse.jetty__jetty-plus__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-webapp--org.eclipse.jetty__jetty-webapp__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.3.20.v20170531.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.fusesource.leveldbjni--leveldbjni-all--org.fusesource.leveldbjni__leveldbjni-all__1.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.hk2.external--javax.inject--org.glassfish.hk2.external__javax.inject__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.jersey.bundles.repackaged--jersey-guava--org.glassfish.jersey.bundles.repackaged__jersey-guava__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.jersey.core--jersey-client--org.glassfish.jersey.core__jersey-client__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.jersey.core--jersey-server--org.glassfish.jersey.core__jersey-server__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.glassfish.jersey.media--jersey-media-jaxb--org.glassfish.jersey.media__jersey-media-jaxb__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.javassist--javassist--org.javassist__javassist__3.18.1-GA.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.jodd--jodd-core--org.jodd__jodd-core__3.5.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.json4s--json4s-ast_2.11--org.json4s__json4s-ast_2.11__3.2.11.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.json4s--json4s-core_2.11--org.json4s__json4s-core_2.11__3.2.11.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.json4s--json4s-jackson_2.11--org.json4s__json4s-jackson_2.11__3.2.11.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.lz4--lz4-java--org.lz4__lz4-java__1.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.mariadb.jdbc--mariadb-java-client--org.mariadb.jdbc__mariadb-java-client__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.mockito--mockito-all--org.mockito__mockito-all__1.9.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.objenesis--objenesis--org.objenesis__objenesis__2.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.postgresql--postgresql--org.postgresql__postgresql__42.1.4.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.roaringbitmap--RoaringBitmap--org.roaringbitmap__RoaringBitmap__0.5.11.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.rocksdb--rocksdbjni--org.rocksdb__rocksdbjni__5.2.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.rosuda.REngine--REngine--org.rosuda.REngine__REngine__2.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scalacheck--scalacheck_2.11--org.scalacheck__scalacheck_2.11__1.12.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.0.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scala-lang--scala-compiler_2.11--org.scala-lang__scala-compiler__2.11.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scala-lang--scalap_2.11--org.scala-lang__scalap__2.11.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scalanlp--breeze_2.11--org.scalanlp__breeze_2.11__0.13.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scalanlp--breeze-macros_2.11--org.scalanlp__breeze-macros_2.11__0.13.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scala-sbt--test-interface--org.scala-sbt__test-interface__1.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__2.2.6.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.slf4j--jcl-over-slf4j--org.slf4j__jcl-over-slf4j__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.slf4j--jul-to-slf4j--org.slf4j__jul-to-slf4j__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.spark-project.spark--unused--org.spark-project.spark__unused__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.spire-math--spire_2.11--org.spire-math__spire_2.11__0.13.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.spire-math--spire-macros_2.11--org.spire-math__spire-macros_2.11__0.13.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.tukaani--xz--org.tukaani__xz__1.0.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.typelevel--machinist_2.11--org.typelevel__machinist_2.11__0.6.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.typelevel--macro-compat_2.11--org.typelevel__macro-compat_2.11__1.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.xerial--sqlite-jdbc--org.xerial__sqlite-jdbc__3.8.11.2.jar:/databricks/jars/spark--maven-trees--spark_2.3--org.yaml--snakeyaml--org.yaml__snakeyaml__1.16.jar:/databricks/jars/spark--maven-trees--spark_2.3--oro--oro--oro__oro__2.0.8.jar:/databricks/jars/spark--maven-trees--spark_2.3--software.amazon.ion--ion-java--software.amazon.ion__ion-ja\n*** WARNING: skipped 7678 bytes of output ***\n\n (u&apos;spark.r.sql.derby.temp.dir&apos;, u&apos;/tmp/RtmptMaRH5&apos;),\n (u&apos;spark.task.reaper.killTimeout&apos;, u&apos;60s&apos;),\n (u&apos;spark.r.numRBackendThreads&apos;, u&apos;1&apos;),\n (u&apos;spark.hadoop.fs.wasbs.impl.disable.cache&apos;, u&apos;true&apos;),\n (u&apos;spark.hadoop.hive.server2.use.SSL&apos;, u&apos;true&apos;),\n (u&apos;spark.hadoop.fs.abfss.impl.disable.cache&apos;, u&apos;true&apos;),\n (u&apos;spark.sql.hive.metastore.version&apos;, u&apos;0.13.0&apos;),\n (u&apos;spark.databricks.sparkContextId&apos;, u&apos;1089417334014128974&apos;),\n (u&apos;spark.shuffle.service.port&apos;, u&apos;4048&apos;),\n (u&apos;spark.databricks.acl.client&apos;,\n  u&apos;com.databricks.spark.sql.acl.client.SparkSqlAclClient&apos;),\n (u&apos;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&apos;, u&apos;true&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterAvailability&apos;, u&apos;ON_DEMAND&apos;),\n (u&apos;spark.hadoop.hive.warehouse.subdir.inherit.perms&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&apos;, u&apos;0&apos;),\n (u&apos;spark.databricks.clusterUsageTags.sparkVersion&apos;, u&apos;4.3.x-scala2.11&apos;),\n (u&apos;spark.hadoop.hive.server2.keystore.path&apos;,\n  u&apos;/databricks/keys/jetty-ssl-driver-keystore.jks&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterPinned&apos;, u&apos;false&apos;),\n (u&apos;spark.sql.ui.retainedExecutions&apos;, u&apos;100&apos;),\n (u&apos;spark.databricks.acl.provider&apos;,\n  u&apos;com.databricks.sql.acl.ReflectionBackedAclProvider&apos;),\n (u&apos;spark.extraListeners&apos;,\n  u&apos;com.databricks.backend.daemon.driver.DBCEventLoggingListener&apos;),\n (u&apos;spark.executor.memory&apos;, u&apos;4800m&apos;),\n (u&apos;spark.databricks.clusterUsageTags.enableElasticDisk&apos;, u&apos;false&apos;),\n (u&apos;spark.sql.parquet.cacheMetadata&apos;, u&apos;true&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterPythonVersion&apos;, u&apos;2&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterOwnerUserId&apos;,\n  u&apos;2582289542480399&apos;),\n (u&apos;spark.hadoop.fs.adl.impl&apos;, u&apos;com.databricks.adl.AdlFileSystem&apos;),\n (u&apos;spark.scheduler.listenerbus.eventqueue.size&apos;, u&apos;20000&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterNodeType&apos;, u&apos;dev-tier-node&apos;),\n (u&apos;spark.databricks.tahoe.logStore.class&apos;,\n  u&apos;com.databricks.tahoe.store.DelegatingLogStore&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterOwnerOrgId&apos;, u&apos;34373005320942&apos;),\n (u&apos;spark.databricks.cloudProvider&apos;, u&apos;AWS&apos;),\n (u&apos;spark.sql.hive.convertMetastoreParquet&apos;, u&apos;true&apos;),\n (u&apos;spark.executor.id&apos;, u&apos;driver&apos;),\n (u&apos;spark.app.name&apos;, u&apos;Databricks Shell&apos;),\n (u&apos;spark.driver.allowMultipleContexts&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.clusterUsageTags.workerEnvironmentId&apos;,\n  u&apos;default-worker-env&apos;),\n (u&apos;spark.rdd.compress&apos;, u&apos;true&apos;),\n (u&apos;spark.repl.class.outputDir&apos;,\n  u&apos;/tmp/spark-4a0da3bb-e6d5-4868-8265-df7ce20a7cf8&apos;),\n (u&apos;spark.databricks.eventLog.dir&apos;, u&apos;eventlogs&apos;),\n (u&apos;spark.hadoop.spark.thriftserver.customHeadersToProperties&apos;,\n  u&apos;X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url&apos;),\n (u&apos;spark.sql.catalogImplementation&apos;, u&apos;hive&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterCreator&apos;, u&apos;Webapp&apos;),\n (u&apos;spark.speculation&apos;, u&apos;false&apos;),\n (u&apos;spark.hadoop.fs.s3a.multipart.size&apos;, u&apos;10485760&apos;),\n (u&apos;spark.hadoop.databricks.dbfs.client.version&apos;, u&apos;v1&apos;),\n (u&apos;spark.hadoop.hive.server2.session.check.interval&apos;, u&apos;60000&apos;),\n (u&apos;spark.sql.hive.convertCTAS&apos;, u&apos;true&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterName&apos;, u&apos;Quickstart1&apos;),\n (u&apos;spark.hadoop.spark.sql.parquet.output.committer.class&apos;,\n  u&apos;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&apos;),\n (u&apos;spark.metrics.conf&apos;, u&apos;/databricks/spark/conf/metrics.properties&apos;),\n (u&apos;spark.databricks.r.cleanWorkspace&apos;, u&apos;true&apos;),\n (u&apos;spark.hadoop.fs.s3a.fast.upload.default&apos;, u&apos;true&apos;),\n (u&apos;spark.akka.frameSize&apos;, u&apos;256&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterGeneration&apos;, u&apos;0&apos;),\n (u&apos;spark.hadoop.fs.s3a.fast.upload&apos;, u&apos;true&apos;),\n (u&apos;spark.hadoop.fs.s3.impl&apos;, u&apos;com.databricks.s3a.S3AFileSystem&apos;),\n (u&apos;spark.hadoop.fs.abfs.impl.disable.cache&apos;, u&apos;true&apos;),\n (u&apos;spark.hadoop.fs.wasbs.impl&apos;,\n  u&apos;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&apos;),\n (u&apos;spark.hadoop.hive.server2.keystore.password&apos;, u&apos;gb1gQqZ9ZIHS&apos;),\n (u&apos;spark.speculation.multiplier&apos;, u&apos;3&apos;),\n (u&apos;spark.storage.blockManagerTimeoutIntervalMs&apos;, u&apos;300000&apos;),\n (u&apos;spark.databricks.overrideDefaultCommitProtocol&apos;,\n  u&apos;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.clusterUsageTags.driverPublicDns&apos;,\n  u&apos;ec2-54-214-119-210.us-west-2.compute.amazonaws.com&apos;),\n (u&apos;spark.hadoop.parquet.memory.pool.ratio&apos;, u&apos;0.5&apos;),\n (u&apos;spark.sparkr.use.daemon&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterAllTags&apos;,\n  u&apos;[{&quot;key&quot;:&quot;Name&quot;,&quot;value&quot;:&quot;ce1-worker&quot;}]&apos;),\n (u&apos;spark.databricks.clusterUsageTags.driverContainerId&apos;,\n  u&apos;83bf3e9be0ef436fa8c24284411f3248&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterScalingType&apos;, u&apos;fixed_size&apos;),\n (u&apos;spark.hadoop.fs.abfss.impl&apos;,\n  u&apos;shaded.databricks.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterStateMessage&apos;, u&apos;Starting Spark&apos;),\n (u&apos;spark.sql.hive.metastore.jars&apos;, u&apos;/databricks/hive/*&apos;),\n (u&apos;spark.hadoop.databricks.s3commit.client.sslTrustAll&apos;, u&apos;false&apos;),\n (u&apos;spark.hadoop.fs.s3a.threads.max&apos;, u&apos;136&apos;),\n (u&apos;spark.r.backendConnectionTimeout&apos;, u&apos;604800&apos;),\n (u&apos;spark.ui.port&apos;, u&apos;41194&apos;),\n (u&apos;spark.serializer.objectStreamReset&apos;, u&apos;100&apos;),\n (u&apos;spark.sql.sources.commitProtocolClass&apos;,\n  u&apos;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&apos;),\n (u&apos;spark.hadoop.hive.server2.idle.session.timeout&apos;, u&apos;900000&apos;),\n (u&apos;spark.databricks.clusterUsageTags.autoTerminationMinutes&apos;, u&apos;120&apos;),\n (u&apos;spark.databricks.clusterUsageTags.enableDfAcls&apos;, u&apos;false&apos;),\n (u&apos;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&apos;, u&apos;0&apos;),\n (u&apos;spark.shuffle.service.enabled&apos;, u&apos;true&apos;),\n (u&apos;spark.hadoop.fs.s3a.multipart.threshold&apos;, u&apos;104857600&apos;),\n (u&apos;spark.hadoop.fs.s3a.impl&apos;, u&apos;com.databricks.s3a.S3AFileSystem&apos;),\n (u&apos;spark.logConf&apos;, u&apos;true&apos;),\n (u&apos;spark.databricks.clusterUsageTags.enableJobsAutostart&apos;, u&apos;true&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&apos;,\n  u&apos;RDS_DIRECT&apos;),\n (u&apos;eventLog.rolloverIntervalSeconds&apos;, u&apos;3600&apos;),\n (u&apos;spark.shuffle.memoryFraction&apos;, u&apos;0.2&apos;),\n (u&apos;spark.databricks.clusterUsageTags.containerZoneId&apos;, u&apos;us-west-2c&apos;),\n (u&apos;spark.databricks.clusterUsageTags.clusterSpotBidPricePercent&apos;, u&apos;100&apos;),\n (u&apos;spark.files.useFetchCache&apos;, u&apos;false&apos;)]\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### how many cores do we have?"],"metadata":{}},{"cell_type":"code","source":["import multiprocessing\n\nmultiprocessing.cpu_count()"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>8\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["%sh pip install psutil"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Collecting psutil\n  Downloading https://files.pythonhosted.org/packages/e3/58/0eae6e4466e5abf779d7e2b71fac7fba5f59e00ea36ddb3ed690419ccb0f/psutil-5.4.8.tar.gz (422kB)\nBuilding wheels for collected packages: psutil\n  Running setup.py bdist_wheel for psutil: started\n  Running setup.py bdist_wheel for psutil: finished with status &apos;done&apos;\n  Stored in directory: /root/.cache/pip/wheels/d2/71/40/9c6993129f8cda369d0f21c46a13a6adab7fb1664fe6512551\nSuccessfully built psutil\nInstalling collected packages: psutil\nSuccessfully installed psutil-5.4.8\nYou are using pip version 8.1.1, however version 18.1 is available.\nYou should consider upgrading via the &apos;pip install --upgrade pip&apos; command.\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["import psutil\npsutil.cpu_count(logical=False)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>4\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["## Creating a RDD from a file  : we will use <font color=orange>churn-bigml-80.csv</font>"],"metadata":{}},{"cell_type":"markdown","source":["The most common way of creating an RDD is to load it from a file.\n\n(Spark's `textFile` can handle compressed files directly, as gz)."],"metadata":{}},{"cell_type":"code","source":["%sh pwd"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/driver\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["data_file = \"/FileStore/tables/churn_bigml_80-bf1a8.csv\"\nraw_data = sc.textFile(data_file)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["Now we have our data file loaded into the `raw_data` RDD."],"metadata":{}},{"cell_type":"markdown","source":["Without getting into Spark *transformations* and *actions*, the most basic thing we can do to check that we got our RDD contents right is to `count()` the number of lines loaded from the file into the RDD."],"metadata":{}},{"cell_type":"code","source":["raw_data.count()"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">15</span><span class=\"ansired\">]: </span>2667\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["We can also check the first few entries in our data."],"metadata":{}},{"cell_type":"code","source":["raw_data.take(5)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">20</span><span class=\"ansired\">]: </span>\n[u&apos;State,Account length,Area code,International plan,Voice mail plan,Number vmail messages,Total day minutes,Total day calls,Total day charge,Total eve minutes,Total eve calls,Total eve charge,Total night minutes,Total night calls,Total night charge,Total intl minutes,Total intl calls,Total intl charge,Customer service calls,Churn&apos;,\n u&apos;KS,128,415,No,Yes,25,265.1,110,45.07,197.4,99,16.78,244.7,91,11.01,10.0,3,2.7,1,False&apos;,\n u&apos;OH,107,415,No,Yes,26,161.6,123,27.47,195.5,103,16.62,254.4,103,11.45,13.7,3,3.7,1,False&apos;,\n u&apos;NJ,137,415,No,No,0,243.4,114,41.38,121.2,110,10.3,162.6,104,7.32,12.2,5,3.29,0,False&apos;,\n u&apos;OH,84,408,Yes,No,0,299.4,71,50.9,61.9,88,5.26,196.9,89,8.86,6.6,7,1.78,2,False&apos;]\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### take first line: first_line\n* ### create a list with features names: features\n* ### print..."],"metadata":{}},{"cell_type":"code","source":["first_line=\n\nfeatures = \nprint(\"number of elements =\",.....)\nprint(\"Features: \\n\",.....)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":24},{"cell_type":"code","source":["first_line=raw_data.take(1)\n\nfeatures = first_line[0].split(\",\")\nprint(\"number of elements =\",len(features))\nprint(\"Features: \\n\",features)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(&apos;number of elements =&apos;, 20)\n(&apos;Features: \\n&apos;, [u&apos;State&apos;, u&apos;Account length&apos;, u&apos;Area code&apos;, u&apos;International plan&apos;, u&apos;Voice mail plan&apos;, u&apos;Number vmail messages&apos;, u&apos;Total day minutes&apos;, u&apos;Total day calls&apos;, u&apos;Total day charge&apos;, u&apos;Total eve minutes&apos;, u&apos;Total eve calls&apos;, u&apos;Total eve charge&apos;, u&apos;Total night minutes&apos;, u&apos;Total night calls&apos;, u&apos;Total night charge&apos;, u&apos;Total intl minutes&apos;, u&apos;Total intl calls&apos;, u&apos;Total intl charge&apos;, u&apos;Customer service calls&apos;, u&apos;Churn&apos;])\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["### Check: pySpark \"strucures\" RDD vs python ... lists, etc."],"metadata":{}},{"cell_type":"code","source":["type(raw_data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">22</span><span class=\"ansired\">]: </span>pyspark.rdd.RDD\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["type(features)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">23</span><span class=\"ansired\">]: </span>list\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["In the following notebooks, we will use this raw data to learn about the different Spark transformations and actions."],"metadata":{}},{"cell_type":"markdown","source":["## Creating and RDD using `parallelize`"],"metadata":{}},{"cell_type":"markdown","source":["Another way of creating an RDD is to parallelize an already existing list."],"metadata":{}},{"cell_type":"markdown","source":["---\n\n* ### Create  range 0,1,2,.... 999\n* ### Parallelize"],"metadata":{}},{"cell_type":"code","source":["a = range(1000)\n\ndata = sc.parallelize(a)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* As we did before, we can count the number of elements in the RDD.\n* and show the first five elements..."],"metadata":{}},{"cell_type":"code","source":["data.count()"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">26</span><span class=\"ansired\">]: </span>1000\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["data....."],"metadata":{"collapsed":false},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["As before, we can access the first few elements on our RDD."],"metadata":{}},{"cell_type":"code","source":["data.take(5)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">27</span><span class=\"ansired\">]: </span>[0, 1, 2, 3, 4]\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["data....."],"metadata":{"collapsed":false},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["## <font color=brown>Let's see now TRANSFORMATIONS and ACTIONS</font>"],"metadata":{}},{"cell_type":"markdown","source":["## The `filter` transformation"],"metadata":{}},{"cell_type":"markdown","source":["This transformation can be applied to RDDs in order to keep just elements that satisfy a certain condition. More concretely, a function is evaluated on every element in the original RDD. The new resulting RDD will contain just those elements that make the function return `True`."],"metadata":{}},{"cell_type":"markdown","source":["For example, imagine we want to count how many lines contain the word `True` in our dataset. We can filter our `raw_data` RDD as follows."],"metadata":{}},{"cell_type":"markdown","source":["## ... We are going to use a `lambda` function"],"metadata":{}},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### Define a function `contrainsTrue` that receives a list and returns True or False depending on if the string True is present in the list\n\n* ### Use a lambda function for the same task..."],"metadata":{}},{"cell_type":"code","source":["def containsTrue(s):\n    return 'True' in s\n\ncontainsTrue(['1','True'])"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">30</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":46},{"cell_type":"code","source":["def ...\n\n....\n\ncontainsTrue(['1','True'])"],"metadata":{"collapsed":false},"outputs":[],"execution_count":47},{"cell_type":"code","source":["f = lambda x : 'True' in x\n\nf(['1','True'])"],"metadata":{"collapsed":false},"outputs":[],"execution_count":48},{"cell_type":"code","source":["f = ....\n\n\nf(['1','True'])"],"metadata":{"collapsed":false},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### FILTER RDD raw_data depending on if the string True is present in its rows using `lambda` function"],"metadata":{}},{"cell_type":"code","source":["True_raw_data = raw_data.filter(lambda x: 'True' in x)"],"metadata":{"collapsed":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":51},{"cell_type":"code","source":["True_raw_data = raw_data.filter(........)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["### ... `count` is the ACTION\n\nNow we can count how many elements we have in the new RDD."],"metadata":{}},{"cell_type":"code","source":["from time import time\nt0 = time()\nTrue_count = True_raw_data.count()\ntt = time() - t0\nprint(\"There are {} lines containing 'True' word\".format(True_count))\nprint(\"Count completed in {} seconds\".format(round(tt,3)))"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">There are 388 lines containing &apos;True&apos; word\nCount completed in 0.235 seconds\n</div>"]}}],"execution_count":54},{"cell_type":"markdown","source":["Notice that we have measured the elapsed time for counting the elements in the RDD. We have done this because we wanted to point out that actual (distributed) computations in Spark take place when we execute *actions* and not *transformations*. In this case `count` is the action we execute on the RDD. We can apply as many transformations as we want on a our RDD and no computation will take place until we call the first action that, in this case takes a few seconds to complete.\n<font color='green' size=4>And this is the \"Lazy Evaluation\"!!</font>"],"metadata":{}},{"cell_type":"markdown","source":["### ** RDD Operations and Lazy Evaluation ** ###\n\n### <font color='green'>Lazy Evaluation</font> ###\n##### Lazy evaluation reduces the number of passes to take over data by grouping operations together. #####\n##### - In MapReduce systems like Hadoop, developers often have to spend a lot time considering how to group together operations to minimize the number of MapReduce passes. #####\n###### - In Spark, there is no substantial benefit to writing a single complex map instead of chaining together many simple operations. Thus, users are free to organize their program into smaller, more manageable operations. #####"],"metadata":{}},{"cell_type":"code","source":["# In this simple example the name of the Churn file is wrong (800 instead of 80)\n#\n#  select lines in log.txt with \"error\" messages\n\ndata_file_err = '/resources/data/MSTC/churn-bigml-800.csv'\nraw_data_err = sc.textFile(data_file_err)\n\nTrue_raw_data_err = raw_data_err.filter(lambda x: 'True' in x)\n\n# but you will get NO error when executing this cell!  as only a transformation (i.e. filter()) is applied"],"metadata":{"collapsed":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":["# Error shows up NOW when performing actions (in this case .count()) (beacause Lazy eval)\n\nTrue_count = True_raw_data_err.count()\n\n# see error traces below:\n# Input path does not exist: file: .... nb2-rdd-basics/kddcup.data_1000_percent.gz"],"metadata":{"collapsed":false},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["## The `map` transformation"],"metadata":{}},{"cell_type":"markdown","source":["By using the `map` transformation in Spark, we can apply a function to every element in our RDD. Python's lambdas are specially expressive for this particular."],"metadata":{}},{"cell_type":"markdown","source":["In this case we want to read our data file as a CSV formatted one. We can do this by applying a lambda function to each element in the RDD as follows."],"metadata":{}},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### MAP RDD raw_data `splitting` each row by comas using `lambda` function"],"metadata":{}},{"cell_type":"code","source":["csv_data = raw_data.map(lambda x: x.split(\",\"))\n\n# see or print some csv_data rows...\ncsv_data.take(3)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":63},{"cell_type":"code","source":["csv_data = raw_data......\n\n# see or print some csv_data rows...\n...."],"metadata":{"collapsed":false},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### Just inser MAP RDD raw_data `splitting`"],"metadata":{}},{"cell_type":"code","source":["from time import time\nfrom pprint import pprint\ncsv_data = raw_data.map(lambda x: x.split(\",\"))\nt0 = time()\nhead_rows = csv_data.take(5)\ntt = time() - t0\nprint(\"Parse completed in {} seconds\".format(round(tt,3)))\npprint(head_rows[0])"],"metadata":{"collapsed":false},"outputs":[],"execution_count":66},{"cell_type":"code","source":["from time import time\nfrom pprint import pprint\ncsv_data = ....\nt0 = time()\nhead_rows = csv_data.take(5)\ntt = time() - t0\nprint(\"Parse completed in {} seconds\".format(round(tt,3)))\npprint(head_rows[0])"],"metadata":{"collapsed":false},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["## The `collect` action"],"metadata":{}},{"cell_type":"markdown","source":["So far we have used the actions `count` and `take`. Another basic action we need to learn is `collect`. Basically it will get all the elements in the RDD into memory for us to work with them. For this reason it has to be used with care, specially when working with large RDDs."],"metadata":{}},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### Calleect all RDD raw_data into all_raw_data \n<br>\n\n\n<font color='red' size=4>NOTE: (This may be dangerous when large datasets!!!) </font>"],"metadata":{}},{"cell_type":"code","source":["t0 = time()\nall_raw_data = raw_data.collect()\ntt = time() - t0\nprint(\"Data collected in {} seconds\".format(round(tt,3)))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":71},{"cell_type":"code","source":["t0 = time()\nall_raw_data = ......\ntt = time() - t0\nprint(\"Data collected in {} seconds\".format(round(tt,3)))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["That COULD took longer as any other action we used before, of course. Every Spark worker node that has a fragment of the RDD has to be coordinated in order to retrieve its part, and then *reduce* everything together."],"metadata":{}},{"cell_type":"markdown","source":["## As a last example: try combining all the previous steps....\n\nTo be defined by 4th december 2017"],"metadata":{}},{"cell_type":"code","source":["# get data from file\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":75}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":"3"},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"MSTC_Spark-rdd-intro (1)","notebookId":1159867712339730,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"anaconda-cloud":{},"widgets":{"state":{},"version":"1.1.2"}},"nbformat":4,"nbformat_minor":0}
