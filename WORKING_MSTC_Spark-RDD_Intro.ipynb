{"cells":[{"cell_type":"markdown","source":["# Introduction to <font color=blue>pySpark</font> https://spark.apache.org/\n\n![Image of Sklearn](https://spark.apache.org/images/spark-logo-trademark.png)"],"metadata":{}},{"cell_type":"markdown","source":["## RDD creation"],"metadata":{}},{"cell_type":"markdown","source":["**Resilient Distributed Dataset** or **RDD**. An RDD is a distributed collection of elements.\n\n* Creating new RDDs, **transforming** existing RDDs, or calling **actions** on RDDs to compute a result.\n\n* Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them."],"metadata":{}},{"cell_type":"markdown","source":["### FIRST check SparkContext is available"],"metadata":{}},{"cell_type":"code","source":["sc"],"metadata":{"collapsed":false},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["#### From Spark 2.2... only *SparkSession*"],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### GET SparkContext configuration will be \"local\" see spark.master\n\n* ### <font color=red>local[*]</font> means Spark locally with as many worker threads as logical cores on your machine."],"metadata":{}},{"cell_type":"code","source":["sc._conf.getAll()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Some checks</font>\n\n* ### how many cores do we have?"],"metadata":{}},{"cell_type":"code","source":["import multiprocessing\n\nmultiprocessing.cpu_count()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sh pip install psutil"],"metadata":{"collapsed":false},"outputs":[],"execution_count":12},{"cell_type":"code","source":["import psutil\npsutil.cpu_count(logical=False)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Creating a RDD from a file  : we will use <font color=orange>churn-bigml-80.csv</font>"],"metadata":{}},{"cell_type":"markdown","source":["The most common way of creating an RDD is to load it from a file.\n\n(Spark's `textFile` can handle compressed files directly, as gz)."],"metadata":{}},{"cell_type":"code","source":["%sh pwd"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## TO DO:\n\n- ### READ churn-bigml-80.csv file as a text File into an RDD : name it RDD_raw_data"],"metadata":{}},{"cell_type":"code","source":["data_file = ???\n\nRDD_raw_data = ???"],"metadata":{"collapsed":false},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Now we have our data file loaded into the `RDD_raw_data` RDD."],"metadata":{}},{"cell_type":"markdown","source":["Without getting into Spark *transformations* and *actions*, the most basic thing we can do to check that we got our RDD contents right is to `count()` the number of lines loaded from the file into the RDD."],"metadata":{}},{"cell_type":"code","source":["???"],"metadata":{"collapsed":false},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["We can also check the first few entries in our data."],"metadata":{}},{"cell_type":"code","source":["???\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### take first line: first_line\n* ### create a list with features names: features\n* ### print..."],"metadata":{}},{"cell_type":"code","source":["first_line= ???\n\nfeatures = ???\nprint \"number of elements =\", ???\nprint \"Features: \\n\", ???"],"metadata":{"collapsed":false},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### NOTE: pySpark \"structures\" RDD vs python ... lists, etc."],"metadata":{}},{"cell_type":"code","source":["type(RDD_raw_data)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["type(features)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## Creating and RDD using `parallelize`"],"metadata":{}},{"cell_type":"markdown","source":["Another way of creating an RDD is to parallelize an already existing list."],"metadata":{}},{"cell_type":"markdown","source":["## TO DO:\n\n* ### from a =  range 0,1,2,.... 999\n* ### create an RDD_data .... (use parallelize)"],"metadata":{}},{"cell_type":"code","source":["a = range(1000)\n\nRDD_data = ???"],"metadata":{"collapsed":false},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* As we did before, we can count the number of elements in the RDD.\n* and show the first five elements..."],"metadata":{}},{"cell_type":"code","source":["???"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["As before, we can access the first 5 elements on our RDD."],"metadata":{}},{"cell_type":"code","source":["???"],"metadata":{"collapsed":false},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["## <font color=brown>Let's see now TRANSFORMATIONS and ACTIONS</font>"],"metadata":{}},{"cell_type":"markdown","source":["## The `filter` transformation"],"metadata":{}},{"cell_type":"markdown","source":["This transformation can be applied to RDDs in order to keep just elements that satisfy a certain condition. More concretely, a function is evaluated on every element in the original RDD. The new resulting RDD will contain just those elements that make the function return `True`."],"metadata":{}},{"cell_type":"markdown","source":["For example, imagine we want to count how many lines contain the word `True` in our dataset. We can filter our `RDD_raw_data` RDD as follows."],"metadata":{}},{"cell_type":"markdown","source":["## ... We are going to use a `lambda` function"],"metadata":{}},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### Define a functionnamed `contrainsTrue` that receives a Python list and returns True or False depending on if the string True is present in the list"],"metadata":{}},{"cell_type":"code","source":["def ???\n\n....\n\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["* ### NOW use a *lambda* function for the same task..."],"metadata":{}},{"cell_type":"code","source":["f = ....\n\n\nf(['1','True'])"],"metadata":{"collapsed":false},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### Do a *TRANSFORMATION* FILTER `RDD_raw_data` depending on if the string True is present in its rows using `lambda` function"],"metadata":{}},{"cell_type":"code","source":["True_raw_data = ???"],"metadata":{"collapsed":true},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["### Now DO the *ACTION* `count` \n\nNow we can count how many elements we have in RDD_raw_data"],"metadata":{}},{"cell_type":"code","source":["True_count = ???\n\nprint(\"There are {} lines containing 'True' word\".format(True_count))"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["Notice that we have measured the elapsed time for counting the elements in the RDD. We have done this because we wanted to point out that actual (distributed) computations in Spark take place when we execute *actions* and not *transformations*. In this case `count` is the action we execute on the RDD. We can apply as many transformations as we want on a our RDD and no computation will take place until we call the first action that, in this case takes a few seconds to complete.\n<font color='green' size=4>And this is the \"Lazy Evaluation\"!!</font>"],"metadata":{}},{"cell_type":"markdown","source":["### ** RDD Operations and Lazy Evaluation ** ###\n\n### <font color='green'>Lazy Evaluation</font> ###\n##### Lazy evaluation reduces the number of passes to take over data by grouping operations together. #####\n##### - In MapReduce systems like Hadoop, developers often have to spend a lot time considering how to group together operations to minimize the number of MapReduce passes. #####\n###### - In Spark, there is no substantial benefit to writing a single complex map instead of chaining together many simple operations. Thus, users are free to organize their program into smaller, more manageable operations. #####"],"metadata":{}},{"cell_type":"code","source":["# In this simple example the name of the Churn file is wrong (800 instead of 80)\n#\n#  select lines in log.txt with \"error\" messages\n\ndata_file_err = \"/FileStore/tables/churn_bigml_80-ZZ.csv\" # File location and type\n\n#churn_bigml_80-bf1a8.csv\n\nRDD_raw_data_err = sc.textFile(data_file_err)\n\n\nTrue_raw_data_err = RDD_raw_data_err.filter(lambda x: 'True' in x)\n\n# but you will get NO error when executing this cell!  as only a transformation (i.e. filter()) is applied"],"metadata":{"collapsed":true},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# Error shows up NOW when performing actions (in this case .count()) (beacause Lazy eval)\n\nTrue_count = True_raw_data_err.count()\n\n# see error traces below:\n# Input path does not exist: file: ......"],"metadata":{"collapsed":false},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["## The `map` transformation"],"metadata":{}},{"cell_type":"markdown","source":["By using the `map` transformation in Spark, we can apply a function to every element in our RDD. Python's lambdas are specially expressive for this particular."],"metadata":{}},{"cell_type":"markdown","source":["In this case we want to read our data file as a CSV formatted one. We can do this by applying a lambda function to each element in the RDD as follows."],"metadata":{}},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### MAP `RDD_raw_data` splitting each row by comas using `lambda` function"],"metadata":{}},{"cell_type":"code","source":["RDD_csv_data = ???\n\n# see or print some csv_data rows...\n???"],"metadata":{"collapsed":false},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["## The `collect` action (warning: you must take care!!!)"],"metadata":{}},{"cell_type":"markdown","source":["So far we have used the actions `count` and `take`. Another basic action we need to learn is `collect`. Basically it will get all the elements in the RDD into memory for us to work with them.\n\n## For this reason it has to be used with care, specially when working with large RDDs."],"metadata":{}},{"cell_type":"markdown","source":["## <font color='magenta'>--------------------------To Do:---------------------------</font>\n\n* ### Collect all RDD_raw_data into all_raw_data \n<br>\n\n\n<font color='red' size=4>NOTE: (This may be dangerous when large datasets!!!) </font>"],"metadata":{}},{"cell_type":"code","source":["all_raw_data = ???\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":62},{"cell_type":"code","source":["len(all_raw_data[0])"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["all_raw_data[0]"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["That COULD took longer as any other action we used before, of course. Every Spark worker node that has a fragment of the RDD has to be coordinated in order to retrieve its part, and then *reduce* everything together."],"metadata":{}},{"cell_type":"markdown","source":["## TO DO:\n\n- ### As a last example: try combining all the previous steps:\n\n##### 1. read csv\n##### 2. filter for 'True'\n##### 3. split by ','\n##### 4. collect (!!!)"],"metadata":{}},{"cell_type":"code","source":["# get data from file\n\n\ndata_file_err = ???\n\n\nfilter_splitted_raw_data = ???\n\n\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":67},{"cell_type":"code","source":["len(filter_splitted_raw_data)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["filter_splitted_raw_data"],"metadata":{},"outputs":[],"execution_count":69}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":"3"},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"MSTC_Spark-rdd-intro (1)","notebookId":1338846609458440,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"anaconda-cloud":{},"widgets":{"state":{},"version":"1.1.2"}},"nbformat":4,"nbformat_minor":0}
