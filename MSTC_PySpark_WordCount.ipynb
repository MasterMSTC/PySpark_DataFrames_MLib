{"cells":[{"cell_type":"markdown","source":["# Let's try to understand Spark a little bit using PySpark<br> and the classical *Word Count* example\n\n# Some references:\n\n## [Holden Karau](http://www.bigdataspain.org/2017/speakers/holden-karau/)\n\n- http://youtu.be/Wg2boMqLjCg \n- https://www.youtube.com/watch?v=4xsBQYdHgn8\n- https://www.youtube.com/watch?v=V6DkTVvy9vk\n- https://www.youtube.com/watch?v=vfiJQ7wg81Y\n- https://robertovitillo.com/2015/06/30/spark-best-practices/ <br>\n- https://www.slideshare.net/SparkSummit/getting-the-best-performance-with-pyspark\n\n#### [HandySpark: bringing pandas-like capabilities to Spark DataFrames](https://towardsdatascience.com/handyspark-bringing-pandas-like-capabilities-to-spark-dataframes-5f1bcea9039e)"],"metadata":{}},{"cell_type":"markdown","source":["![Distrubuted Spark](http://www.bogotobogo.com/Hadoop/images/PySpark/ComponentsForDistributedExecutionInSpark.png)\n\n![PySpark Python](https://www.packtpub.com/graphics/9781786463708/graphics/B05793_03_01.jpg )"],"metadata":{}},{"cell_type":"markdown","source":["## Word Count Example\n- ### <font color=  2e5f54 size=6 face=\"verdana\">Sparkâ€™s simplicity makes it all too easy to ignore its execution model and still manage to write jobs that eventually complete.\n- ### With larger datasets having an understanding of what happens under the hood becomes critical to reduce run-time and avoid out of memory errors</font>\n\n### RDD operations are compiled into a Direct Acyclic Graph of RDD objects, where each RDD points to the parent it depends on:"],"metadata":{}},{"cell_type":"markdown","source":["![DAG](https://raw.githubusercontent.com/MasterMSTC/PySpark_DataFrames_MLib/master/images/image1.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["![Direct Acyclic Graph of RDD objects](https://ravitillo.files.wordpress.com/2015/06/dag1.png)"],"metadata":{}},{"cell_type":"markdown","source":["## Best practices\n\n## Spark UI\n\n- ###<font color=red size=4 face=\"verdana\">Running Spark jobs without the **Spark UI** is like flying blind.\n- ### The UI allows to monitor and inspect the execution of jobs.\n- ### To access it remotely a SOCKS proxy is needed as the UI connects also to the worker nodes."],"metadata":{}},{"cell_type":"markdown","source":["## Let's try: Word Count Example"],"metadata":{}},{"cell_type":"markdown","source":["## <font color= 187b1a>Word count using RDD: reduceByKey? groupByKey?</font>"],"metadata":{}},{"cell_type":"code","source":["%sh ls /dbfs/databricks-datasets"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%sh cat /dbfs/databricks-datasets/README.md"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## <font color= 187b1a>Word count using RDD: reduceByKey</font>"],"metadata":{}},{"cell_type":"markdown","source":["- ## TO DO: cread rdd_lines from a text file `file:/dbfs/databricks-datasets/README.md `"],"metadata":{}},{"cell_type":"code","source":["# rdd_lines = ???"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["rdd_lines = sc.textFile(\"file:/dbfs/databricks-datasets/README.md\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["rdd_lines.count()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["- ## TO DO: count how how many times each line of text occurs"],"metadata":{}},{"cell_type":"code","source":["# ???"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# NOT THIS: \n\nrdd_lines.count()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["pairs = rdd_lines.map(lambda s: (s, 1))\ncounts = pairs.reduceByKey(lambda a, b: a + b)\n\n#pairs.take(5)\ncounts.collect()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["- ## TO DO: to count how how many WORDS  ... we need an rdd_words !!! (splited by words!)"],"metadata":{}},{"cell_type":"code","source":["# rdd_words = ???"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["\n\nrdd_words = rdd_lines.flatMap(lambda line: line.split())"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["rdd_words.take(5)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## <font color=C70039>We can download some larger text files</font>"],"metadata":{}},{"cell_type":"code","source":["import os\n\nfrom six.moves import urllib\n\n#file_url = 'http://www.gutenberg.org/cache/epub/2000/pg2000.txt'\n#file_name = '/resources/data/MSTC/cervantes.txt'\n\n#file_url = 'https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt'\n#file_name = '/resources/data/MSTC/t8.shakespeare.txt'\n\n# NOTE that compressed files can be read as simple txt : NOTHING particular must be done!\nfile_url='http://ftp.sunet.se/mirror/archive/ftp.sunet.se/pub/tv+movies/imdb/producers.list.gz'\nfile_name = 'producers.list.gz'\n    \nif not os.path.exists(file_name):\n    urllib.request.urlretrieve(file_url, file_name)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sh\ngunzip producers.list.gz\nls -al"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["- ## TO DO (Group 1): Word count using RDD: reduceByKey"],"metadata":{}},{"cell_type":"code","source":["#rdd_lines = sc.textFile(\"file:/dbfs/databricks-datasets/README.md\")\nrdd_lines = sc.textFile(\"file:/databricks/driver/producers.list\")\n#rdd_lines = sc.textFile(\"file:/dbfs/databricks-datasets/README.md\")\n# rdd_lines = sc.textFile(\"file:/databricks/driver/producers.list.gz\")\n\nrdd_words = rdd_lines.flatMap(lambda line: line.split())\n\nrdd_word_pairs = rdd_words.map(lambda x: (x, 1))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["rdd_word_pairs.take(10)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["#rdd_lines = sc.textFile(\"file:/dbfs/databricks-datasets/README.md\")\nrdd_lines = sc.textFile(\"file:/databricks/driver/producers.list\")\n#rdd_lines = sc.textFile(\"file:/dbfs/databricks-datasets/README.md\")\n# rdd_lines = sc.textFile(\"file:/databricks/driver/producers.list.gz\")\n\n#rdd_words = rdd_lines.flatMap(lambda line: line.split())\n\n#rdd_word_pairs = rdd_words.map(lambda x: (x, 1))\n\n#word_count = rdd_word_pairs.reduceByKey(lambda x, y : x + y)\n\nword_count = rdd_lines.flatMap(lambda line: line.split()).map(lambda x: (x, 1)). \\\n                      reduceByKey(lambda x, y : x + y)\n\nword_count.collect()\n\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## <font color= 187b1a>Word count using RDD: groupByKey</font>"],"metadata":{}},{"cell_type":"code","source":["# ???"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["rdd_lines = sc.textFile(\"file:/databricks/driver/producers.list\")\n#rdd_lines = sc.textFile(\"file:/dbfs/databricks-datasets/README.md\")\n\nrdd_words = rdd_lines.flatMap(lambda line: line.split())\n\nrdd_word_pairs = rdd_words.map(lambda x: (x, 1))\n\nrdd_groups = rdd_word_pairs.groupByKey()\n\n#rdd_counted_words = rdd_groups.mapValues(lambda counts: sum(counts))\n\nrdd_counted_words = rdd_groups.map(lambda (w, counts): (w, sum(counts)))\n\nrdd_counted_words.collect()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["## <font color=  #7b1864 >Word count using DataFrames:</font>\n### without ordering the results...\n### BUT see first PySpark Dataframes notebook"],"metadata":{}},{"cell_type":"code","source":["# ???"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# http://wpcertification.blogspot.com/2016/07/wordcount-program-using-spark-dataframe.html?utm_source=twitterfeed&utm_medium=twitter\n\nimport pyspark\nimport pyspark.sql.functions as f\n\ndf = sqlContext.read.text(\"file:/databricks/driver/producers.list\")\n#df = sqlContext.read.text(\"file:/dbfs/databricks-datasets/README.md\")\n\n#df.show()\n\nwordDF = df.select(f.explode(f.split(df['value'], ' ')).alias(\"words\"))\n\nwordCountDF = wordDF.groupBy(\"words\").count()\n\nwordCountDF.show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### Create and cache a Dataframe with words"],"metadata":{}},{"cell_type":"code","source":["df = sqlContext.read.text(file_name)\n\nwords=df.flatMap(lambda line: line.value.split())\\\n    .map(lambda x:Row(word=x, cnt=1)).toDF()\n    \nwords.cache()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":40},{"cell_type":"code","source":["words.limit(5).toPandas()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":41},{"cell_type":"code","source":["words.count()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":42},{"cell_type":"code","source":["t0 = time()\n\nword_count=words.groupBy('word').count()\\\n    .collect()\n\ntt = time() - t0\nprint(\"Task completed in {} seconds\".format(round(tt,3)))\n    "],"metadata":{"collapsed":false},"outputs":[],"execution_count":43},{"cell_type":"code","source":["word_count[0:5]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["## <font color= 187b1a>Word count using RDD</font>\n### NOW ordering the results..."],"metadata":{}},{"cell_type":"code","source":["t0 = time()\n\n\nrdd_word_count = rdd_words.map(lambda word: (word,1))\\\n    .reduceByKey(lambda x,y: x + y)\\\n    .map(lambda x: (x[1],x[0])) \\\n    .sortByKey(ascending=False) \\\n    .collect()\n    \ntt = time() - t0\nprint(\"Task completed in {} seconds\".format(round(tt,3)))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":46},{"cell_type":"code","source":["rdd_word_count[0:5]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["## <font color=  #7b1864 >Word count using DataFrames:</font>\n### Now ordering the results..."],"metadata":{}},{"cell_type":"code","source":["t0 = time()\n\nword_count=words.groupBy('word').count()\\\n    .orderBy('count',ascending=False).collect()\n\ntt = time() - t0\nprint(\"Task completed in {} seconds\".format(round(tt,3)))\n    "],"metadata":{"collapsed":false},"outputs":[],"execution_count":49},{"cell_type":"code","source":["word_count[0:5]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":50},{"cell_type":"code","source":["\n    "],"metadata":{"collapsed":false},"outputs":[],"execution_count":51},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":52}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"MSTC_PySpark_WordCount","notebookId":1391740554107690,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"widgets":{"state":{},"version":"1.1.2"}},"nbformat":4,"nbformat_minor":0}
