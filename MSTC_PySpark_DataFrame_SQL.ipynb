{"cells":[{"cell_type":"code","source":["displayHTML(\"<font size=8 color='green'>Introduction to Spark Data Frames and SQL using PySpark</font>\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["### [MSTC](http://mstc.ssr.upm.es/big-data-track) and MUIT:"],"metadata":{}},{"cell_type":"markdown","source":["## Sources:\n* [Databriks: introduction-to-dataframes-python](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)\n* [Introduction to Spark with Python, by Jose A. Dianes](http://jadianes.github.io/spark-py-notebooks)\n* [Complete Guide on DataFrame Operations in PySpark](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/)\n* [Understanding-DataFrames](https://github.com/awantik/pyspark-tutorial/wiki/Understanding-DataFrames)\n* [From Pandas to Spark Dataframes](https://github.com/awantik/pyspark-tutorial/wiki/Migrating-from-Pandas-to-Apache-Spark%E2%80%99s-DataFrame)\n* [Also ML](https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/)"],"metadata":{}},{"cell_type":"markdown","source":["## This notebook will introduce Spark capabilities to deal with data in a structured way.\n* ### Basically, everything turns around the concept of *Data Frame* and using *SQL language* to query them.</font>\")"],"metadata":{}},{"cell_type":"markdown","source":["## In Apache Spark, a DataFrame is a **distributed collection of rows under named columns**.\n- ### In simple terms, it is same as a table in relational database or an Excel sheet with Column headers.\n\n## It also shares some common characteristics with RDD:<br>\n\n*    **Immutable** in nature : We can create DataFrame / RDD once but canâ€™t change it. And we can transform a DataFrame / RDD after applying transformations.\n*    **Lazy Evaluations**: Which means that a task is not executed until an action is performed.\n*    **Distributed**: RDD and DataFrame both are distributed in nature."],"metadata":{}},{"cell_type":"markdown","source":["### PERFORMANCE:"],"metadata":{}},{"cell_type":"markdown","source":["![How to create a DataFrame](https://camo.githubusercontent.com/cc93c064c6fd754df0209d42ec054998edd81fa0/68747470733a2f2f7777772e736166617269626f6f6b736f6e6c696e652e636f6d2f6c6962726172792f766965772f6c6561726e696e672d7079737061726b2f393738313738363436333730382f67726170686963732f4230353739335f30335f30332e6a7067)"],"metadata":{}},{"cell_type":"markdown","source":["## How to create a DataFrame ?\n \n ![How to create a DataFrame](https://www.analyticsvidhya.com/wp-content/uploads/2016/10/DataFrame-in-Spark.png)"],"metadata":{}},{"cell_type":"markdown","source":["* ### A Spark `DataFrame` is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R or Pandas. They can be constructed from a wide array of sources such as a existing RDD in our case."],"metadata":{}},{"cell_type":"markdown","source":["## <font color=#AA1B5A> DataFrame RDD of Row objects\n\nFrom: http://www.cs.sfu.ca/CourseCentral/732/ggbaker/content/spark-sql.html"],"metadata":{}},{"cell_type":"markdown","source":["### Think of a DataFrame being implemented with an RDD of Row objects.\n- ### Row is a generic row object with an ordered collection of field\n- ### Nicest way to create Rows: create a custom subclass for your data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nNameAge = Row('fname lname', 'age') # build a Row subclass\n\nuser1 = NameAge('John Smith', 47)\nuser2 = NameAge('Jane Smith', 22)\nuser3 = NameAge('Frank Jones', 28)\n\ndata_rows = [ user1, user2, user3 ]\n\nprint(data_rows)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df1 = spark.createDataFrame(data_rows)\n\ndf1.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Databricks DISPLAY\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## TO DO: create another DataFrame df2 with sames users but with their weights:\n\nfname lname|  weight\n\n- 'John Smith' 80.5\n- 'Jane Smith' 62.3\n- 'Frank Jones' 71.5"],"metadata":{}},{"cell_type":"code","source":["NameWeight = Row('fname lname', 'weight') # build a Row subclass\n\ndf2 =  spark.createDataFrame([NameWeight('John Smith', 80.5), \n                              NameWeight('Jane Smith', 62.3),\n                              NameWeight('Frank Jones', 71.5)])\n\ndisplay(df2)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## TO DO: Join both DataFrames into df"],"metadata":{}},{"cell_type":"code","source":["df = df1.join(df2, \"fname lname\")\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## We can apply functions to Columns using `pyspark.sql.functions` or our own Used-Definded Functions (UDF)\n\n### for example:\n\n- 1.- `select(\\*cols)` : Projects a set of expressions and returns a new DataFrame.<br>\n- 2.- apply `split` function to the \"fname lname\" column : split fname and lname\n- 3.- `alias` returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode)"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\ndf_new= df.select(f.split(df['fname lname'],' ').alias('sep names'))\n\ndf_new.show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["- ## `explode(col)`: this function returns a new row for each element in the given array or map."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\ndf_new = df.select(f.explode(f.split(df['fname lname'],' ')).alias('all'))\n\ndf_new.show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["# Creating a Data Frame from CSV file"],"metadata":{}},{"cell_type":"markdown","source":["## <font color=#F01B5A>We will read our Orange Churn dataset"],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/churn_bigml_80-bf1a8.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["type(df)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":26},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display(df.describe())"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Convert to a Date type\ndf = df.withColumn('Voice mail plan', f.regexp_replace(df['Voice mail plan'],'Yes','1'))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["df.count()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":31},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["## `groupby`:\n* ### How to find Churn vs no_Churn cases?"],"metadata":{}},{"cell_type":"code","source":["df.groupby('Churn').count().show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["df.crosstab('State', 'Churn').show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["dc=df.groupBy(\"State\").agg(f.count(\"Churn\").alias('Num Churn'))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["dc.show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["## Use `filter()` to return the rows that match a predicate"],"metadata":{}},{"cell_type":"code","source":["filterDF = df.filter( df.State == \"CA\" )\n#filterDF = df.filter( (df.State == \"CA\") & (df.Churn == 'False') )\n#filterDF = df.filter( (df.State == \"CA\") & (df['Total day calls'] >  90) )\n\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["filterDF.count()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["countDistinctDF = df.select(\"State\", \"Churn\")\\\n  .groupBy(\"State\")\\\n  .agg(f.countDistinct(\"Churn\"))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["countDistinctDF.show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["# Spark SQL schema"],"metadata":{}},{"cell_type":"markdown","source":["## For using Spark SQL we need the schema in our data."],"metadata":{}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["## COLUMNS?\n\n## <font color=#F81B5A>...worth mentioning PARQUET\n\n![Parquet](https://parquet.apache.org/assets/img/parquet_logo.png)\nhttps://parquet.apache.org/\n\n### Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language."],"metadata":{}},{"cell_type":"markdown","source":["## Before SQL Note that you can also convert freely between Pandas DataFrame and Spark DataFrame</font>"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"collapsed":true},"outputs":[],"execution_count":48},{"cell_type":"code","source":["pd.DataFrame(df.take(5), columns=df.columns)"],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["## or..."],"metadata":{}},{"cell_type":"code","source":["df.toPandas().head(5)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":51},{"cell_type":"code","source":["CV_data.groupby('Churn').agg({'Customer service calls': 'mean'}).show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["### <font color=#F81BA0 size=5>TO DO:</font>\n\n- ### How to find the mean of 'Customer service calls' in every state"],"metadata":{}},{"cell_type":"code","source":["df.groupby('State').agg({'Total day minutes': 'mean', 'Customer service calls': 'mean'}).toPandas()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":54},{"cell_type":"code","source":["CV_data.groupby('State').agg({'Total day minutes': 'mean', 'Customer service calls': 'mean'}).toPandas()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["# <font color=#F81B5A>SQL Syntax\n\n## There is also a spark.sql function where you can do the same things with SQL query syntax."],"metadata":{}},{"cell_type":"markdown","source":["### Apply SQL Queries on DataFrame\n\n* ### <font color=brown>To apply SQL queries on DataFrame first we need to register DataFrame as table. Letâ€™s first register train DataFrame as table."],"metadata":{}},{"cell_type":"code","source":["df.registerTempTable('df_table')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":58},{"cell_type":"code","source":["Mean_DayMin_ServiceCalls = sqlContext.sql(\"\"\"\n    SELECT State, MEAN(`Total day minutes`), MEAN(`Customer service calls`) \n    FROM df_table GROUP BY State\n\"\"\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":59},{"cell_type":"code","source":["type(Mean_DayMin_ServiceCalls)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["Mean_DayMin_ServiceCalls.show()"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["Mean_DayMin_ServiceCalls.toPandas()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["### <font color=red>...NOW order: descend by average Day Minutes"],"metadata":{}},{"cell_type":"code","source":["Day_min = sqlContext.sql(\"\"\"\n    SELECT State, MEAN(`Total day minutes`) as average_DayMin, MEAN(`Customer service calls`) \n    FROM df_table GROUP BY State order by average_DayMin desc\n\"\"\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":64},{"cell_type":"code","source":["pd.DataFrame(Day_min.take(5))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["## <font color=#F81B5A>... same as before but using SQL-like methods:"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\nDay_min2=df.groupby('State').agg(f.mean('Total day minutes').alias(\"average_DayMin\")\n                            , f.mean('Customer service calls')) \\\n                            .orderBy(f.desc(\"average_DayMin\"))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":67},{"cell_type":"code","source":["pd.DataFrame(Day_min2.take(5))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["### <font color=brownUDFs> We can register a user defined function (UDF) from Python"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import UserDefinedFunction\n\nbinary_map = {'Yes':1.0, 'No':0.0, 'True':1.0, 'False':0.0}\n\ntoNum = UserDefinedFunction(lambda k: binary_map[k], DoubleType())"],"metadata":{"collapsed":false},"outputs":[],"execution_count":70},{"cell_type":"code","source":["pd.DataFrame(df.take(5), columns=df.columns)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">200</span><span class=\"ansired\">]: </span>\n  State  Account length  Area code International plan Voice mail plan  \\\n0    KS             128        415                 No               1   \n1    OH             107        415                 No               1   \n2    NJ             137        415                 No              No   \n3    OH              84        408                Yes              No   \n4    OK              75        415                Yes              No   \n\n   Number vmail messages  Total day minutes  Total day calls  \\\n0                     25              265.1              110   \n1                     26              161.6              123   \n2                      0              243.4              114   \n3                      0              299.4               71   \n4                      0              166.7              113   \n\n   Total day charge  Total eve minutes  Total eve calls  Total eve charge  \\\n0             45.07              197.4               99             16.78   \n1             27.47              195.5              103             16.62   \n2             41.38              121.2              110             10.30   \n3             50.90               61.9               88              5.26   \n4             28.34              148.3              122             12.61   \n\n   Total night minutes  Total night calls  Total night charge  \\\n0                244.7                 91               11.01   \n1                254.4                103               11.45   \n2                162.6                104                7.32   \n3                196.9                 89                8.86   \n4                186.9                121                8.41   \n\n   Total intl minutes  Total intl calls  Total intl charge  \\\n0                10.0                 3               2.70   \n1                13.7                 3               3.70   \n2                12.2                 5               3.29   \n3                 6.6                 7               1.78   \n4                10.1                 3               2.73   \n\n   Customer service calls  Churn  \n0                       1  False  \n1                       1  False  \n2                       0  False  \n3                       2  False  \n4                       3  False  \n</div>"]}}],"execution_count":71},{"cell_type":"code","source":["df = df.withColumn('Churn', toNum(df['Churn'])) \\\n    .withColumn('International plan', toNum(df['International plan'])) \\\n    .withColumn('Voice mail plan', toNum(df['Voice mail plan']))"],"metadata":{"collapsed":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":72},{"cell_type":"markdown","source":["### <font color=red>...NOTE that you MUST assign CV_data = ... to a NEW dataFrame"],"metadata":{}},{"cell_type":"code","source":["df = df.drop('Voice mail plan2')"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":74},{"cell_type":"code","source":["df.columns"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">204</span><span class=\"ansired\">]: </span>\n[&apos;State&apos;,\n &apos;Account length&apos;,\n &apos;Area code&apos;,\n &apos;International plan&apos;,\n &apos;Voice mail plan&apos;,\n &apos;Number vmail messages&apos;,\n &apos;Total day minutes&apos;,\n &apos;Total day calls&apos;,\n &apos;Total day charge&apos;,\n &apos;Total eve minutes&apos;,\n &apos;Total eve calls&apos;,\n &apos;Total eve charge&apos;,\n &apos;Total night minutes&apos;,\n &apos;Total night calls&apos;,\n &apos;Total night charge&apos;,\n &apos;Total intl minutes&apos;,\n &apos;Total intl calls&apos;,\n &apos;Total intl charge&apos;,\n &apos;Customer service calls&apos;,\n &apos;Churn&apos;]\n</div>"]}}],"execution_count":75},{"cell_type":"code","source":["pd.DataFrame(df.take(5), columns=df.columns)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-2709831939590155&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>pd<span class=\"ansiyellow\">.</span>DataFrame<span class=\"ansiyellow\">(</span>df<span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">5</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> columns<span class=\"ansiyellow\">=</span>df<span class=\"ansiyellow\">.</span>columns<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">take</span><span class=\"ansiblue\">(self, num)</span>\n<span class=\"ansigreen\">    520</span>         <span class=\"ansiyellow\">[</span>Row<span class=\"ansiyellow\">(</span>age<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;Alice&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>age<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">5</span><span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;Bob&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    521</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 522</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>limit<span class=\"ansiyellow\">(</span>num<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    523</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    524</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1.3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    479</span>         <span class=\"ansired\"># Default path used in OSS Spark / for non-DF-ACL clusters:</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    480</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 481</span><span class=\"ansiyellow\">             </span>sock_info <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>collectToPython<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    482</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>sock_info<span class=\"ansiyellow\">,</span> BatchedSerializer<span class=\"ansiyellow\">(</span>PickleSerializer<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    483</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o2964.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 460.0 failed 1 times, most recent failure: Lost task 0.0 in stage 460.0 (TID 9091, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 262, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 257, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 183, in &lt;lambda&gt;\n    func = lambda _, it: map(mapper, it)\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 79, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 55, in wrapper\n    return f(*args, **kwargs)\n  File &quot;&lt;command-2709831939590149&gt;&quot;, line 6, in &lt;lambda&gt;\nKeyError: u&apos;1&apos;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:317)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:271)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:620)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:49)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:126)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:125)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:384)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1747)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1735)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1734)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1734)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:962)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:962)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:962)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1970)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1918)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1906)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2141)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:237)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:247)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:64)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:70)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:497)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:48)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3236)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3234)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3334)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:89)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:175)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:84)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:126)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3333)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3234)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 262, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 257, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 183, in &lt;lambda&gt;\n    func = lambda _, it: map(mapper, it)\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 79, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 55, in wrapper\n    return f(*args, **kwargs)\n  File &quot;&lt;command-2709831939590149&gt;&quot;, line 6, in &lt;lambda&gt;\nKeyError: u&apos;1&apos;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:317)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:271)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:620)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:49)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:126)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:125)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:384)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":76},{"cell_type":"markdown","source":["## `sample`:\n- ###   How to create a sample DataFrame from the base DataFrame?\n\n### The sample method on DataFrame will return a DataFrame containing the sample of base DataFrame. The sample method will take 3 parameters.\n\n- ### withReplacement = True or False to select a observation with or without replacement. fraction = x, where x = .5 shows that we want to have 50% data in sample DataFrame;  seed for reproduce the result\n\n### Letâ€™s create the two DataFrame t1 and t2 from train, both will have 20% sample of train and count the number of rows in each."],"metadata":{}},{"cell_type":"code","source":["t1 = df.sample(False, 0.5, 42)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":78},{"cell_type":"code","source":["t1.count()"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">208</span><span class=\"ansired\">]: </span>1349\n</div>"]}}],"execution_count":79},{"cell_type":"markdown","source":["## `appy`: apply map operation on DataFrame columns\n\nWe can apply a function on each row of DataFrame using map operation. After applying this function, we get the result in the form of RDD. Letâ€™s apply a map operation on User_ID column of train and print the first 5 elements of mapped RDD(x,1) after applying the function (I am applying lambda function)."],"metadata":{}},{"cell_type":"markdown","source":["## RETURN TO: Notebook with Word Count Example"],"metadata":{}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"NEW_MSTC_PySpark_DataFrame","notebookId":2709831939590081,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"widgets":{"state":{},"version":"1.1.2"}},"nbformat":4,"nbformat_minor":0}
