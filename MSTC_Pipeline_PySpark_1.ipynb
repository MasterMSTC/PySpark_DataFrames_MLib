{"cells":[{"cell_type":"code","source":["displayHTML(\"<font size=8>Let's start building simple ELEMENTS of a <font size=8 color='green'>PIPELINE</font> for</font> <font color=orange size=8>Orange Churn dataset</font>\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["![How to create a DataFrame](https://blog.cloudera.com/wp-content/uploads/2017/04/Spark.png)"],"metadata":{}},{"cell_type":"markdown","source":["### [MSTC](http://mstc.ssr.upm.es/big-data-track) and MUIT:"],"metadata":{}},{"cell_type":"markdown","source":["## Importing Churn Data\n\n###  Load churn-bigml-80.csv into a DataFrame"],"metadata":{}},{"cell_type":"code","source":["%fs ls /FileStore/tables"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\nCV_data = sqlContext.read.load('/FileStore/tables/churn_bigml_80-bf1a8.csv', \n                          format='com.databricks.spark.csv', \n                          header='true', \n                          inferSchema='true')\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(CV_data)"],"metadata":{"collapsed":false,"pixiedust":{"displayParams":{"keyFields":"Churn","timeseries":"true","rowCount":"500","valueFields":"Customer service calls","chartsize":"100","mpld3":"false","sortby":"Values ASC","rendererId":"matplotlib","aggregation":"AVG","handlerId":"barChart","clusterby":"International plan","orientation":"vertical"}}},"outputs":[],"execution_count":7},{"cell_type":"code","source":["CV_data.printSchema()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["This is simply to illustrate an example to apply a UDF to a Spark DataFrame"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType, StringType\nfrom pyspark.sql.functions import UserDefinedFunction\n\ntoStr = UserDefinedFunction(lambda k: k, StringType())\nCV_data = CV_data.withColumn('Churn', toStr(CV_data['Churn']))\n\n#binary_map = {'Yes':1.0, 'No':0.0, 'True':1.0, 'False':0.0}\n#toNum = UserDefinedFunction(lambda k: binary_map[k], DoubleType())\n#CV_data = CV_data.withColumn('Churn', toNum(CV_data['Churn']))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["CV_data.printSchema()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Spark: ML Pipelines\nhttps://spark.apache.org/docs/latest/ml-pipeline.html"],"metadata":{}},{"cell_type":"markdown","source":["##  <font color= #e38009> Transformer A: StringIndexer\n  https://spark.apache.org/docs/latest/ml-features.html#stringindexer\n\n<font font-family: \"calibri\" size=3.5>StringIndexer converts String values that are part of a look-up into categorical indices, which could be used by machine learning algorithms in ml library.\n\n***Notice we provide the input column name and the output column name as parameters at the time of initialization of the StringIndexer.***"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\n# Index labels: using StringIndexer to encodes a string column of labels Churn (\"True\" , \"False\" strings NO Boolean) to a column of label indices indexedChurn\n\nstringindexer = StringIndexer(inputCol='Churn',\n                             outputCol='indexedChurn')\n\nmodel=stringindexer.fit(CV_data)\n\n\ndataframe_transformedA=model.transform(CV_data)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(dataframe_transformedA)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["##  <font color= #e38009> Transformer B: VectorAssembler\n\n### ...after “feature engineering” … the feature engineering results are then combined using the VectorAssembler, before being passed to ML Estimator"],"metadata":{}},{"cell_type":"markdown","source":["###  For simplicity: first we drop all columns:\n* categorical\n* and numerical highly correlated"],"metadata":{}},{"cell_type":"markdown","source":["### This will be our list with predictors"],"metadata":{"collapsed":false}},{"cell_type":"code","source":["predictors=('Number vmail messages',\n 'Total day minutes',\n 'Total day calls',\n 'Total eve minutes',\n 'Total eve calls',\n 'Total night minutes',\n 'Total night calls',\n 'Total intl minutes',\n 'Total intl calls',\n 'Customer service calls')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#### Notice we provide to *VectorAssembler* the input = list of columns (MUST BE NUMERIC!) and the output column assembles all of them in a single column/vector"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nassembler=VectorAssembler(inputCols=predictors,outputCol='features')\n\ndataframe_transformedB=assembler.transform(dataframe_transformedA).select('indexedChurn','features')\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":21},{"cell_type":"code","source":["dataframe_transformedB.take(5)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["##  <font color=#FF5733> Estimators\n\n<font font-family: \"calibri\" size=3.5>\nAn Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. \n\nTechnically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer. <br><br>\n***For example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.***"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n\n# Train a DecisionTree model\ndTree_algorithm = DecisionTreeClassifier(maxDepth=2,\n                                        labelCol='indexedChurn', featuresCol='features')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":["dTree_model=dTree_algorithm.fit(dataframe_transformedB)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":25},{"cell_type":"code","source":["print(dTree_model._call_java(\"toDebugString\"))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["##  <font color= #e38009> Transformers include:learned models: \n\n*** e.g.  take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column***"],"metadata":{}},{"cell_type":"code","source":["predictions=dTree_model.transform(dataframe_transformedB)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":28},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":29},{"cell_type":"code","source":["display(predictions)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["import pandas as pd\n\npd.DataFrame(predictions.take(5), columns=predictions.columns)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["## <font color=#938882>Model Evaluation\n\n### *** For evaluation we will use the training cvs file, that is Train Error***"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator"],"metadata":{"collapsed":false},"outputs":[],"execution_count":33},{"cell_type":"code","source":["evaluator=BinaryClassificationEvaluator(labelCol='indexedChurn',\\\n                                        rawPredictionCol='rawPrediction',\\\n                                       metricName='areaUnderROC')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":34},{"cell_type":"code","source":["accuracy=evaluator.evaluate(predictions)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":35},{"cell_type":"code","source":["accuracy"],"metadata":{"collapsed":false},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# Since dTree_model is a Model (i.e., a transformer produced by an Estimator),\n# we can view the parameters it used during fit().\n# This prints the parameter (name: value) pairs, where names are unique IDs for this\n# LogisticRegression instance.\nprint(\"dTree_model was fit using parameters: \")\nprint(dTree_model.extractParamMap())"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["dTree_model.extractParamMap().keys()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["dTree_model.maxDepth"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">95</span><span class=\"ansired\">]: </span>Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;maxDepth&apos;, doc=&apos;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&apos;)\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":["# We may alternatively specify parameters using a Python dictionary as a paramMap\nparamMap = {dTree_model.maxDepth: 1}\nparamMap[dTree_model.maxDepth] = 7  # Specify 1 Param, overwriting the original maxIter.\n\n\n# Now learn a new model using the paramMap parameters.\n# paramMap overrides all parameters set earlier via dTree_model.set* methods.\ndTree_model2=dTree_algorithm.fit(dataframe_transformedB, paramMap)\n\ndTree_model2.extractParamMap()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">108</span><span class=\"ansired\">]: </span>\n{Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;checkpointInterval&apos;, doc=&apos;set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext&apos;): 10,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;featuresCol&apos;, doc=&apos;features column name&apos;): &apos;features&apos;,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;cacheNodeIds&apos;, doc=&apos;If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.&apos;): False,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;impurity&apos;, doc=&apos;Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini&apos;): &apos;gini&apos;,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;maxBins&apos;, doc=&apos;Max number of bins for discretizing continuous features.  Must be &gt;=2 and &gt;= number of categories for any categorical feature.&apos;): 32,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;labelCol&apos;, doc=&apos;label column name&apos;): &apos;indexedChurn&apos;,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;maxMemoryInMB&apos;, doc=&apos;Maximum memory in MB allocated to histogram aggregation.&apos;): 256,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;maxDepth&apos;, doc=&apos;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&apos;): 7,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;minInfoGain&apos;, doc=&apos;Minimum information gain for a split to be considered at a tree node.&apos;): 0.0,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;predictionCol&apos;, doc=&apos;prediction column name&apos;): &apos;prediction&apos;,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;minInstancesPerNode&apos;, doc=&apos;Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be &gt;= 1.&apos;): 1,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;rawPredictionCol&apos;, doc=&apos;raw prediction (a.k.a. confidence) column name&apos;): &apos;rawPrediction&apos;,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;seed&apos;, doc=&apos;random seed&apos;): 6174923023070228847,\n Param(parent=u&apos;DecisionTreeClassifier_47c98c8c31663453d066&apos;, name=&apos;probabilityCol&apos;, doc=&apos;Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities&apos;): &apos;probability&apos;}\n</div>"]}}],"execution_count":40},{"cell_type":"code","source":["predictions2=dTree_model2.transform(dataframe_transformedB)\n\naccuracy2=evaluator.evaluate(predictions2)\nprint(accuracy2)\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.711933623272\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["print(dTree_model2._call_java(\"toDebugString\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DecisionTreeClassificationModel (uid=DecisionTreeClassifier_47c98c8c31663453d066) of depth 7 with 139 nodes\n  If (feature 1 &lt;= 263.1)\n   If (feature 9 &lt;= 3.5)\n    If (feature 1 &lt;= 220.85000000000002)\n     If (feature 7 &lt;= 13.05)\n      If (feature 8 &lt;= 2.5)\n       If (feature 3 &lt;= 259.65)\n        If (feature 4 &lt;= 103.5)\n         Predict: 0.0\n        Else (feature 4 &gt; 103.5)\n         Predict: 0.0\n       Else (feature 3 &gt; 259.65)\n        If (feature 6 &lt;= 77.5)\n         Predict: 1.0\n        Else (feature 6 &gt; 77.5)\n         Predict: 0.0\n      Else (feature 8 &gt; 2.5)\n       If (feature 3 &lt;= 183.35000000000002)\n        If (feature 2 &lt;= 73.5)\n         Predict: 0.0\n        Else (feature 2 &gt; 73.5)\n         Predict: 0.0\n       Else (feature 3 &gt; 183.35000000000002)\n        If (feature 8 &lt;= 6.5)\n         Predict: 0.0\n        Else (feature 8 &gt; 6.5)\n         Predict: 0.0\n     Else (feature 7 &gt; 13.05)\n      If (feature 0 &lt;= 38.5)\n       If (feature 8 &lt;= 17.5)\n        If (feature 9 &lt;= 0.5)\n         Predict: 0.0\n        Else (feature 9 &gt; 0.5)\n         Predict: 0.0\n       Else (feature 8 &gt; 17.5)\n        Predict: 1.0\n      Else (feature 0 &gt; 38.5)\n       If (feature 6 &lt;= 88.5)\n        Predict: 1.0\n       Else (feature 6 &gt; 88.5)\n        If (feature 1 &lt;= 175.64999999999998)\n         Predict: 0.0\n        Else (feature 1 &gt; 175.64999999999998)\n         Predict: 1.0\n    Else (feature 1 &gt; 220.85000000000002)\n     If (feature 3 &lt;= 240.64999999999998)\n      If (feature 7 &lt;= 13.850000000000001)\n       If (feature 8 &lt;= 2.5)\n        If (feature 6 &lt;= 129.5)\n         Predict: 0.0\n        Else (feature 6 &gt; 129.5)\n         Predict: 1.0\n       Else (feature 8 &gt; 2.5)\n        If (feature 5 &lt;= 253.55)\n         Predict: 0.0\n        Else (feature 5 &gt; 253.55)\n         Predict: 0.0\n      Else (feature 7 &gt; 13.850000000000001)\n       If (feature 3 &lt;= 220.85000000000002)\n        If (feature 6 &lt;= 113.5)\n         Predict: 0.0\n        Else (feature 6 &gt; 113.5)\n         Predict: 1.0\n       Else (feature 3 &gt; 220.85000000000002)\n        Predict: 1.0\n     Else (feature 3 &gt; 240.64999999999998)\n      If (feature 0 &lt;= 2.0)\n       If (feature 5 &lt;= 149.95)\n        If (feature 8 &lt;= 1.5)\n         Predict: 1.0\n        Else (feature 8 &gt; 1.5)\n         Predict: 0.0\n       Else (feature 5 &gt; 149.95)\n        If (feature 7 &lt;= 6.65)\n         Predict: 0.0\n        Else (feature 7 &gt; 6.65)\n         Predict: 1.0\n      Else (feature 0 &gt; 2.0)\n       If (feature 7 &lt;= 15.350000000000001)\n        Predict: 0.0\n       Else (feature 7 &gt; 15.350000000000001)\n        Predict: 1.0\n   Else (feature 9 &gt; 3.5)\n    If (feature 1 &lt;= 179.95)\n     If (feature 3 &lt;= 212.14999999999998)\n      If (feature 5 &lt;= 226.55)\n       Predict: 1.0\n      Else (feature 5 &gt; 226.55)\n       If (feature 3 &lt;= 183.35000000000002)\n        If (feature 1 &lt;= 171.05)\n         Predict: 1.0\n        Else (feature 1 &gt; 171.05)\n         Predict: 0.0\n       Else (feature 3 &gt; 183.35000000000002)\n        Predict: 0.0\n     Else (feature 3 &gt; 212.14999999999998)\n      If (feature 1 &lt;= 153.0)\n       If (feature 8 &lt;= 4.5)\n        If (feature 5 &lt;= 278.8)\n         Predict: 1.0\n        Else (feature 5 &gt; 278.8)\n         Predict: 0.0\n       Else (feature 8 &gt; 4.5)\n        If (feature 1 &lt;= 138.2)\n         Predict: 1.0\n        Else (feature 1 &gt; 138.2)\n         Predict: 0.0\n      Else (feature 1 &gt; 153.0)\n       Predict: 0.0\n    Else (feature 1 &gt; 179.95)\n     If (feature 3 &lt;= 141.95)\n      If (feature 4 &lt;= 95.5)\n       Predict: 1.0\n      Else (feature 4 &gt; 95.5)\n       If (feature 5 &lt;= 155.95)\n        Predict: 1.0\n       Else (feature 5 &gt; 155.95)\n        If (feature 6 &lt;= 106.5)\n         Predict: 0.0\n        Else (feature 6 &gt; 106.5)\n         Predict: 1.0\n     Else (feature 3 &gt; 141.95)\n      If (feature 6 &lt;= 115.5)\n       If (feature 9 &lt;= 6.5)\n        If (feature 6 &lt;= 82.5)\n         Predict: 0.0\n        Else (feature 6 &gt; 82.5)\n         Predict: 0.0\n       Else (feature 9 &gt; 6.5)\n        Predict: 1.0\n      Else (feature 6 &gt; 115.5)\n       If (feature 2 &lt;= 120.5)\n        If (feature 8 &lt;= 3.5)\n         Predict: 1.0\n        Else (feature 8 &gt; 3.5)\n         Predict: 0.0\n       Else (feature 2 &gt; 120.5)\n        Predict: 1.0\n  Else (feature 1 &gt; 263.1)\n   If (feature 0 &lt;= 2.0)\n    If (feature 3 &lt;= 187.9)\n     If (feature 1 &lt;= 279.1)\n      If (feature 5 &lt;= 253.55)\n       If (feature 2 &lt;= 77.5)\n        If (feature 2 &lt;= 69.5)\n         Predict: 0.0\n        Else (feature 2 &gt; 69.5)\n         Predict: 1.0\n       Else (feature 2 &gt; 77.5)\n        Predict: 0.0\n      Else (feature 5 &gt; 253.55)\n       If (feature 3 &lt;= 123.45)\n        Predict: 0.0\n       Else (feature 3 &gt; 123.45)\n        Predict: 1.0\n     Else (feature 1 &gt; 279.1)\n      If (feature 8 &lt;= 4.5)\n       If (feature 7 &lt;= 5.05)\n        Predict: 0.0\n       Else (feature 7 &gt; 5.05)\n        Predict: 1.0\n      Else (feature 8 &gt; 4.5)\n       If (feature 7 &lt;= 10.05)\n        Predict: 0.0\n       Else (feature 7 &gt; 10.05)\n        If (feature 4 &lt;= 100.5)\n         Predict: 1.0\n        Else (feature 4 &gt; 100.5)\n         Predict: 0.0\n    Else (feature 3 &gt; 187.9)\n     If (feature 5 &lt;= 123.1)\n      If (feature 6 &lt;= 103.5)\n       If (feature 7 &lt;= 8.45)\n        Predict: 0.0\n       Else (feature 7 &gt; 8.45)\n        Predict: 1.0\n      Else (feature 6 &gt; 103.5)\n       Predict: 0.0\n     Else (feature 5 &gt; 123.1)\n      If (feature 6 &lt;= 63.5)\n       If (feature 2 &lt;= 105.5)\n        Predict: 1.0\n       Else (feature 2 &gt; 105.5)\n        Predict: 0.0\n      Else (feature 6 &gt; 63.5)\n       Predict: 1.0\n   Else (feature 0 &gt; 2.0)\n    If (feature 6 &lt;= 124.5)\n     If (feature 3 &lt;= 252.6)\n      If (feature 8 &lt;= 8.5)\n       Predict: 0.0\n      Else (feature 8 &gt; 8.5)\n       If (feature 0 &lt;= 29.5)\n        Predict: 1.0\n       Else (feature 0 &gt; 29.5)\n        Predict: 0.0\n     Else (feature 3 &gt; 252.6)\n      If (feature 7 &lt;= 8.45)\n       If (feature 0 &lt;= 26.5)\n        Predict: 0.0\n       Else (feature 0 &gt; 26.5)\n        Predict: 1.0\n      Else (feature 7 &gt; 8.45)\n       Predict: 0.0\n    Else (feature 6 &gt; 124.5)\n     If (feature 0 &lt;= 13.5)\n      Predict: 0.0\n     Else (feature 0 &gt; 13.5)\n      Predict: 1.0\n\n</div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["##  Model selection via cross-validation\n\nIn this example we will use CrossValidator to select from a grid of parameters in the Tree model"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n# Search through decision tree's maxDepth parameter for best model\nparamGrid = ParamGridBuilder().addGrid(dTree_algorithm.maxDepth, [2,3,4,5,6,7]).build()"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":44},{"cell_type":"code","source":["# Set up 3-fold cross validation\ncrossval = CrossValidator(estimator=dTree_algorithm,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)"],"metadata":{"collapsed":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":45},{"cell_type":"code","source":["Cross_res=crossval.fit(dataframe_transformedB)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":46},{"cell_type":"code","source":["print(Cross_res.bestModel)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DecisionTreeClassificationModel (uid=DecisionTreeClassifier_47c98c8c31663453d066) of depth 7 with 139 nodes\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":["print(Cross_res.bestModel._call_java(\"toDebugString\"))"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DecisionTreeClassificationModel (uid=DecisionTreeClassifier_47c98c8c31663453d066) of depth 7 with 139 nodes\n  If (feature 1 &lt;= 263.1)\n   If (feature 9 &lt;= 3.5)\n    If (feature 1 &lt;= 220.85000000000002)\n     If (feature 7 &lt;= 13.05)\n      If (feature 8 &lt;= 2.5)\n       If (feature 3 &lt;= 259.65)\n        If (feature 4 &lt;= 103.5)\n         Predict: 0.0\n        Else (feature 4 &gt; 103.5)\n         Predict: 0.0\n       Else (feature 3 &gt; 259.65)\n        If (feature 6 &lt;= 77.5)\n         Predict: 1.0\n        Else (feature 6 &gt; 77.5)\n         Predict: 0.0\n      Else (feature 8 &gt; 2.5)\n       If (feature 3 &lt;= 183.35000000000002)\n        If (feature 2 &lt;= 73.5)\n         Predict: 0.0\n        Else (feature 2 &gt; 73.5)\n         Predict: 0.0\n       Else (feature 3 &gt; 183.35000000000002)\n        If (feature 8 &lt;= 6.5)\n         Predict: 0.0\n        Else (feature 8 &gt; 6.5)\n         Predict: 0.0\n     Else (feature 7 &gt; 13.05)\n      If (feature 0 &lt;= 38.5)\n       If (feature 8 &lt;= 17.5)\n        If (feature 9 &lt;= 0.5)\n         Predict: 0.0\n        Else (feature 9 &gt; 0.5)\n         Predict: 0.0\n       Else (feature 8 &gt; 17.5)\n        Predict: 1.0\n      Else (feature 0 &gt; 38.5)\n       If (feature 6 &lt;= 88.5)\n        Predict: 1.0\n       Else (feature 6 &gt; 88.5)\n        If (feature 1 &lt;= 175.64999999999998)\n         Predict: 0.0\n        Else (feature 1 &gt; 175.64999999999998)\n         Predict: 1.0\n    Else (feature 1 &gt; 220.85000000000002)\n     If (feature 3 &lt;= 240.64999999999998)\n      If (feature 7 &lt;= 13.850000000000001)\n       If (feature 8 &lt;= 2.5)\n        If (feature 6 &lt;= 129.5)\n         Predict: 0.0\n        Else (feature 6 &gt; 129.5)\n         Predict: 1.0\n       Else (feature 8 &gt; 2.5)\n        If (feature 5 &lt;= 253.55)\n         Predict: 0.0\n        Else (feature 5 &gt; 253.55)\n         Predict: 0.0\n      Else (feature 7 &gt; 13.850000000000001)\n       If (feature 3 &lt;= 220.85000000000002)\n        If (feature 6 &lt;= 113.5)\n         Predict: 0.0\n        Else (feature 6 &gt; 113.5)\n         Predict: 1.0\n       Else (feature 3 &gt; 220.85000000000002)\n        Predict: 1.0\n     Else (feature 3 &gt; 240.64999999999998)\n      If (feature 0 &lt;= 2.0)\n       If (feature 5 &lt;= 149.95)\n        If (feature 8 &lt;= 1.5)\n         Predict: 1.0\n        Else (feature 8 &gt; 1.5)\n         Predict: 0.0\n       Else (feature 5 &gt; 149.95)\n        If (feature 7 &lt;= 6.65)\n         Predict: 0.0\n        Else (feature 7 &gt; 6.65)\n         Predict: 1.0\n      Else (feature 0 &gt; 2.0)\n       If (feature 7 &lt;= 15.350000000000001)\n        Predict: 0.0\n       Else (feature 7 &gt; 15.350000000000001)\n        Predict: 1.0\n   Else (feature 9 &gt; 3.5)\n    If (feature 1 &lt;= 179.95)\n     If (feature 3 &lt;= 212.14999999999998)\n      If (feature 5 &lt;= 226.55)\n       Predict: 1.0\n      Else (feature 5 &gt; 226.55)\n       If (feature 3 &lt;= 183.35000000000002)\n        If (feature 1 &lt;= 171.05)\n         Predict: 1.0\n        Else (feature 1 &gt; 171.05)\n         Predict: 0.0\n       Else (feature 3 &gt; 183.35000000000002)\n        Predict: 0.0\n     Else (feature 3 &gt; 212.14999999999998)\n      If (feature 1 &lt;= 153.0)\n       If (feature 8 &lt;= 4.5)\n        If (feature 5 &lt;= 278.8)\n         Predict: 1.0\n        Else (feature 5 &gt; 278.8)\n         Predict: 0.0\n       Else (feature 8 &gt; 4.5)\n        If (feature 1 &lt;= 138.2)\n         Predict: 1.0\n        Else (feature 1 &gt; 138.2)\n         Predict: 0.0\n      Else (feature 1 &gt; 153.0)\n       Predict: 0.0\n    Else (feature 1 &gt; 179.95)\n     If (feature 3 &lt;= 141.95)\n      If (feature 4 &lt;= 95.5)\n       Predict: 1.0\n      Else (feature 4 &gt; 95.5)\n       If (feature 5 &lt;= 155.95)\n        Predict: 1.0\n       Else (feature 5 &gt; 155.95)\n        If (feature 6 &lt;= 106.5)\n         Predict: 0.0\n        Else (feature 6 &gt; 106.5)\n         Predict: 1.0\n     Else (feature 3 &gt; 141.95)\n      If (feature 6 &lt;= 115.5)\n       If (feature 9 &lt;= 6.5)\n        If (feature 6 &lt;= 82.5)\n         Predict: 0.0\n        Else (feature 6 &gt; 82.5)\n         Predict: 0.0\n       Else (feature 9 &gt; 6.5)\n        Predict: 1.0\n      Else (feature 6 &gt; 115.5)\n       If (feature 2 &lt;= 120.5)\n        If (feature 8 &lt;= 3.5)\n         Predict: 1.0\n        Else (feature 8 &gt; 3.5)\n         Predict: 0.0\n       Else (feature 2 &gt; 120.5)\n        Predict: 1.0\n  Else (feature 1 &gt; 263.1)\n   If (feature 0 &lt;= 2.0)\n    If (feature 3 &lt;= 187.9)\n     If (feature 1 &lt;= 279.1)\n      If (feature 5 &lt;= 253.55)\n       If (feature 2 &lt;= 77.5)\n        If (feature 2 &lt;= 69.5)\n         Predict: 0.0\n        Else (feature 2 &gt; 69.5)\n         Predict: 1.0\n       Else (feature 2 &gt; 77.5)\n        Predict: 0.0\n      Else (feature 5 &gt; 253.55)\n       If (feature 3 &lt;= 123.45)\n        Predict: 0.0\n       Else (feature 3 &gt; 123.45)\n        Predict: 1.0\n     Else (feature 1 &gt; 279.1)\n      If (feature 8 &lt;= 4.5)\n       If (feature 7 &lt;= 5.05)\n        Predict: 0.0\n       Else (feature 7 &gt; 5.05)\n        Predict: 1.0\n      Else (feature 8 &gt; 4.5)\n       If (feature 7 &lt;= 10.05)\n        Predict: 0.0\n       Else (feature 7 &gt; 10.05)\n        If (feature 4 &lt;= 100.5)\n         Predict: 1.0\n        Else (feature 4 &gt; 100.5)\n         Predict: 0.0\n    Else (feature 3 &gt; 187.9)\n     If (feature 5 &lt;= 123.1)\n      If (feature 6 &lt;= 103.5)\n       If (feature 7 &lt;= 8.45)\n        Predict: 0.0\n       Else (feature 7 &gt; 8.45)\n        Predict: 1.0\n      Else (feature 6 &gt; 103.5)\n       Predict: 0.0\n     Else (feature 5 &gt; 123.1)\n      If (feature 6 &lt;= 63.5)\n       If (feature 2 &lt;= 105.5)\n        Predict: 1.0\n       Else (feature 2 &gt; 105.5)\n        Predict: 0.0\n      Else (feature 6 &gt; 63.5)\n       Predict: 1.0\n   Else (feature 0 &gt; 2.0)\n    If (feature 6 &lt;= 124.5)\n     If (feature 3 &lt;= 252.6)\n      If (feature 8 &lt;= 8.5)\n       Predict: 0.0\n      Else (feature 8 &gt; 8.5)\n       If (feature 0 &lt;= 29.5)\n        Predict: 1.0\n       Else (feature 0 &gt; 29.5)\n        Predict: 0.0\n     Else (feature 3 &gt; 252.6)\n      If (feature 7 &lt;= 8.45)\n       If (feature 0 &lt;= 26.5)\n        Predict: 0.0\n       Else (feature 0 &gt; 26.5)\n        Predict: 1.0\n      Else (feature 7 &gt; 8.45)\n       Predict: 0.0\n    Else (feature 6 &gt; 124.5)\n     If (feature 0 &lt;= 13.5)\n      Predict: 0.0\n     Else (feature 0 &gt; 13.5)\n      Predict: 1.0\n\n</div>"]}}],"execution_count":48},{"cell_type":"code","source":["# Fetch the best model for make predictions with it:\nBest_tree_model = Cross_res.bestModel\nprint(Best_tree_model)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DecisionTreeClassificationModel (uid=DecisionTreeClassifier_47c98c8c31663453d066) of depth 7 with 139 nodes\n</div>"]}}],"execution_count":49},{"cell_type":"code","source":["predictions_CV=Best_tree_model.transform(dataframe_transformedB)"],"metadata":{"collapsed":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":50},{"cell_type":"code","source":["pd.DataFrame(predictions_CV.take(5), columns=predictions.columns)"],"metadata":{"collapsed":false,"scrolled":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">118</span><span class=\"ansired\">]: </span>\n   indexedChurn                                           features  \\\n0           0.0  [25.0, 265.1, 110.0, 197.4, 99.0, 244.7, 91.0,...   \n1           0.0  [26.0, 161.6, 123.0, 195.5, 103.0, 254.4, 103....   \n2           0.0  [0.0, 243.4, 114.0, 121.2, 110.0, 162.6, 104.0...   \n3           0.0  [0.0, 299.4, 71.0, 61.9, 88.0, 196.9, 89.0, 6....   \n4           0.0  [0.0, 166.7, 113.0, 148.3, 122.0, 186.9, 121.0...   \n\n   rawPrediction                         probability  prediction  \n0    [29.0, 0.0]                          [1.0, 0.0]         0.0  \n1  [190.0, 18.0]   [0.913461538462, 0.0865384615385]         0.0  \n2   [159.0, 4.0]   [0.975460122699, 0.0245398773006]         0.0  \n3     [5.0, 0.0]                          [1.0, 0.0]         0.0  \n4   [461.0, 4.0]  [0.991397849462, 0.00860215053763]         0.0  \n</div>"]}}],"execution_count":51},{"cell_type":"code","source":["accuracy_CV=evaluator.evaluate(predictions_CV)\n\nprint(accuracy_CV)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.711933623272\n</div>"]}}],"execution_count":52},{"cell_type":"markdown","source":["## Now let's create a PIPELINE! see MSTC_Pipeline_PySpark_2.ipynb"],"metadata":{"collapsed":true}},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":54}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"MSTC_Pipeline_PySpark_1","notebookId":2515846536831973,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"widgets":{"state":{},"version":"1.1.2"}},"nbformat":4,"nbformat_minor":0}
