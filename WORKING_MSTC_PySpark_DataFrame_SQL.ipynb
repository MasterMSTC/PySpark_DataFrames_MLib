{"cells":[{"cell_type":"code","source":["displayHTML(\"<font size=8 color='green'>Introduction to Spark Data Frames and SQL using PySpark</font>\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["### [MSTC](http://mstc.ssr.upm.es/big-data-track) and MUIT:"],"metadata":{}},{"cell_type":"markdown","source":["## Sources:\n* [Databriks: introduction-to-dataframes-python](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)\n* [Introduction to Spark with Python, by Jose A. Dianes](http://jadianes.github.io/spark-py-notebooks)\n* [Complete Guide on DataFrame Operations in PySpark](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/)\n* [Understanding-DataFrames](https://github.com/awantik/pyspark-tutorial/wiki/Understanding-DataFrames)\n* [From Pandas to Spark Dataframes](https://github.com/awantik/pyspark-tutorial/wiki/Migrating-from-Pandas-to-Apache-Spark%E2%80%99s-DataFrame)\n* [Also ML](https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/)"],"metadata":{}},{"cell_type":"markdown","source":["## This notebook will introduce Spark capabilities to deal with data in a structured way.\n* ### Basically, everything turns around the concept of *Data Frame* and using *SQL language* to query them.</font>\")"],"metadata":{}},{"cell_type":"markdown","source":["## In Apache Spark, a DataFrame is a **distributed collection of rows under named columns**.\n- ### In simple terms, it is same as a table in relational database or an Excel sheet with Column headers.\n\n## It also shares some common characteristics with RDD:<br>\n\n*    **Immutable** in nature : We can create DataFrame / RDD once but canâ€™t change it. And we can transform a DataFrame / RDD after applying transformations.\n*    **Lazy Evaluations**: Which means that a task is not executed until an action is performed.\n*    **Distributed**: RDD and DataFrame both are distributed in nature."],"metadata":{}},{"cell_type":"markdown","source":["### PERFORMANCE:"],"metadata":{}},{"cell_type":"markdown","source":["![How to create a DataFrame](https://camo.githubusercontent.com/cc93c064c6fd754df0209d42ec054998edd81fa0/68747470733a2f2f7777772e736166617269626f6f6b736f6e6c696e652e636f6d2f6c6962726172792f766965772f6c6561726e696e672d7079737061726b2f393738313738363436333730382f67726170686963732f4230353739335f30335f30332e6a7067)"],"metadata":{}},{"cell_type":"markdown","source":["## How to create a DataFrame ?\n \n ![How to create a DataFrame](https://www.analyticsvidhya.com/wp-content/uploads/2016/10/DataFrame-in-Spark.png)"],"metadata":{}},{"cell_type":"markdown","source":["* ### A Spark `DataFrame` is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R or Pandas. They can be constructed from a wide array of sources such as a existing RDD in our case."],"metadata":{}},{"cell_type":"markdown","source":["## <font color=#AA1B5A> DataFrame RDD of Row objects\n\nFrom: http://www.cs.sfu.ca/CourseCentral/732/ggbaker/content/spark-sql.html"],"metadata":{}},{"cell_type":"markdown","source":["### Think of a DataFrame being implemented with an RDD of Row objects.\n- ### Row is a generic row object with an ordered collection of field\n- ### Nicest way to create Rows: create a custom subclass for your data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nNameAge = Row('fname lname', 'age') # build a Row subclass\n\nuser1 = NameAge('John Smith', 47)\nuser2 = NameAge('Jane Smith', 22)\nuser3 = NameAge('Frank Jones', 28)\n\ndata_rows = [ user1, user2, user3 ]\n\nprint(data_rows)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df1 = spark.createDataFrame(data_rows)\n\ndf1.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Databricks DISPLAY\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## TO DO: create another DataFrame df2 with sames users but with their weights:\n\nfname lname|  weight\n\n- 'John Smith' 80.5\n- 'Jane Smith' 62.3\n- 'Frank Jones' 71.5"],"metadata":{}},{"cell_type":"code","source":["???"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## TO DO: Join both DataFrames into df"],"metadata":{}},{"cell_type":"code","source":["df = df1.join(df2, \"fname lname\")\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## We can apply functions to Columns using `pyspark.sql.functions` or our own Used-Definded Functions (UDF)\n\n### for example:\n\n- 1.- `select(\\*cols)` : Projects a set of expressions and returns a new DataFrame.<br>\n- 2.- apply `split` function to the \"fname lname\" column : split fname and lname\n- 3.- `alias` returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode)"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\ndf_new= df.select(f.split(df['fname lname'],' ').alias('sep names'))\n\ndf_new.show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["- ## `explode(col)`: this function returns a new row for each element in the given array or map."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\ndf_new = df.select(f.explode(f.split(df['fname lname'],' ')).alias('all'))\n\ndf_new.show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["# Creating a Data Frame from CSV file"],"metadata":{}},{"cell_type":"markdown","source":["## <font color=#F01B5A>We will read our Orange Churn dataset"],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/churn_bigml_80-bf1a8.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["type(df)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":26},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display(df.describe())"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Convert to a Date type\ndf = df.withColumn('Voice mail plan', f.regexp_replace(df['Voice mail plan'],'Yes','1'))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["df.count()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":31},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["## `groupby`:\n* ### How to find Churn vs no_Churn cases?"],"metadata":{}},{"cell_type":"code","source":["df.groupby('Churn').count().show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["df.crosstab('State', 'Churn').show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["dc=df.groupBy(\"State\").agg(f.count(\"Churn\").alias('Num Churn'))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["dc.show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["## Use `filter()` to return the rows that match a predicate"],"metadata":{}},{"cell_type":"code","source":["filterDF = df.filter( df.State == \"CA\" )\n#filterDF = df.filter( (df.State == \"CA\") & (df.Churn == 'False') )\n#filterDF = df.filter( (df.State == \"CA\") & (df['Total day calls'] >  90) )\n\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["filterDF.count()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["countDistinctDF = df.select(\"State\", \"Churn\")\\\n  .groupBy(\"State\")\\\n  .agg(f.countDistinct(\"Churn\"))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["countDistinctDF.show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["# Spark SQL schema"],"metadata":{}},{"cell_type":"markdown","source":["## For using Spark SQL we need the schema in our data."],"metadata":{}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["## COLUMNS?\n\n## <font color=#F81B5A>...worth mentioning PARQUET\n\n![Parquet](https://parquet.apache.org/assets/img/parquet_logo.png)\nhttps://parquet.apache.org/\n\n### Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language."],"metadata":{}},{"cell_type":"markdown","source":["## Before SQL Note that you can also convert freely between Pandas DataFrame and Spark DataFrame</font>"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"collapsed":true},"outputs":[],"execution_count":48},{"cell_type":"code","source":["pd.DataFrame(df.take(5), columns=df.columns)"],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["## or..."],"metadata":{}},{"cell_type":"code","source":["df.toPandas().head(5)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":51},{"cell_type":"code","source":["CV_data.groupby('Churn').agg({'Customer service calls': 'mean'}).show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["### <font color=#F81BA0 size=5>TO DO:</font>\n\n- ### How to find the mean of 'Customer service calls' in every state"],"metadata":{}},{"cell_type":"code","source":["???"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["# <font color=#F81B5A>SQL Syntax\n\n## There is also a spark.sql function where you can do the same things with SQL query syntax."],"metadata":{}},{"cell_type":"markdown","source":["### Apply SQL Queries on DataFrame\n\n* ### <font color=brown>To apply SQL queries on DataFrame first we need to register DataFrame as table. Letâ€™s first register train DataFrame as table."],"metadata":{}},{"cell_type":"code","source":["df.registerTempTable('df_table')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":["Mean_DayMin_ServiceCalls = sqlContext.sql(\"\"\"\n    SELECT State, MEAN(`Total day minutes`), MEAN(`Customer service calls`) \n    FROM df_table GROUP BY State\n\"\"\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":58},{"cell_type":"code","source":["type(Mean_DayMin_ServiceCalls)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["Mean_DayMin_ServiceCalls.show()"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["Mean_DayMin_ServiceCalls.toPandas()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["### <font color=red>...NOW order: descend by average Day Minutes"],"metadata":{}},{"cell_type":"code","source":["Day_min = sqlContext.sql(\"\"\"\n    SELECT State, MEAN(`Total day minutes`) as average_DayMin, MEAN(`Customer service calls`) \n    FROM df_table GROUP BY State order by average_DayMin desc\n\"\"\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":63},{"cell_type":"code","source":["pd.DataFrame(Day_min.take(5))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["## <font color=#F81B5A>... same as before but using SQL-like methods:"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\nDay_min2=df.groupby('State').agg(f.mean('Total day minutes').alias(\"average_DayMin\")\n                            , f.mean('Customer service calls')) \\\n                            .orderBy(f.desc(\"average_DayMin\"))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":66},{"cell_type":"code","source":["pd.DataFrame(Day_min2.take(5))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["### <font color=brownUDFs> We can register a user defined function (UDF) from Python"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import UserDefinedFunction\n\nbinary_map = {'Yes':1.0, 'No':0.0, 'True':1.0, 'False':0.0}\n\ntoNum = UserDefinedFunction(lambda k: binary_map[k], DoubleType())"],"metadata":{"collapsed":false},"outputs":[],"execution_count":69},{"cell_type":"code","source":["pd.DataFrame(df.take(5), columns=df.columns)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":70},{"cell_type":"code","source":["df = df.withColumn('Churn', toNum(df['Churn'])) \\\n    .withColumn('International plan', toNum(df['International plan'])) \\\n    .withColumn('Voice mail plan', toNum(df['Voice mail plan']))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["### <font color=red>...NOTE that you MUST assign CV_data = ... to a NEW dataFrame"],"metadata":{}},{"cell_type":"code","source":["df = df.drop('Voice mail plan2')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":73},{"cell_type":"code","source":["df.columns"],"metadata":{"collapsed":false},"outputs":[],"execution_count":74},{"cell_type":"code","source":["pd.DataFrame(df.take(5), columns=df.columns)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["## `sample`:\n- ###   How to create a sample DataFrame from the base DataFrame?\n\n### The sample method on DataFrame will return a DataFrame containing the sample of base DataFrame. The sample method will take 3 parameters.\n\n- ### withReplacement = True or False to select a observation with or without replacement. fraction = x, where x = .5 shows that we want to have 50% data in sample DataFrame;  seed for reproduce the result\n\n### Letâ€™s create the two DataFrame t1 and t2 from train, both will have 20% sample of train and count the number of rows in each."],"metadata":{}},{"cell_type":"code","source":["t1 = df.sample(False, 0.5, 42)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":77},{"cell_type":"code","source":["t1.count()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["## `appy`: apply map operation on DataFrame columns\n\nWe can apply a function on each row of DataFrame using map operation. After applying this function, we get the result in the form of RDD. Letâ€™s apply a map operation on User_ID column of train and print the first 5 elements of mapped RDD(x,1) after applying the function (I am applying lambda function)."],"metadata":{}},{"cell_type":"markdown","source":["## RETURN TO: Notebook with Word Count Example"],"metadata":{}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"NEW_MSTC_PySpark_DataFrame","notebookId":2709831939590551,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"widgets":{"state":{},"version":"1.1.2"}},"nbformat":4,"nbformat_minor":0}
